---
title: Run Locally with Ollama
description: The fastest local path: install Ollama and run an SLM.
---

# Run locally with Ollama

## Install

Follow Ollama’s official install instructions for your OS.

## Run a model

```bash
ollama run phi4
```

If that exact model name doesn’t exist in your environment:

- search available models in your Ollama registry, or
- pick a model from `models/featured/` and adapt the name.

## Next step: expose an API

Go to `learn/tutorials/serve-openai-compatible.md`.


