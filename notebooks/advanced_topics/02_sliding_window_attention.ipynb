{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4.2: Sliding Window Attention\n\n**Goal**: Implement sliding window attention for efficient long sequences\n\n**Time**: 75 minutes\n\n**Concepts Covered**:\n- Sliding window attention implementation\n- Compare full vs windowed attention\n- Measure effective context length (L \u00d7 W)\n- Benchmark speed and memory\n- Visualize attention patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install torch transformers accelerate matplotlib seaborn numpy -q"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef sliding_window_attention(Q, K, V, window_size=512, causal=True):\n    \"\"\"Sliding Window Attention\n    \n    Args:\n        Q, K, V: Query, Key, Value matrices (batch, seq_len, d_k)\n        window_size: Size of attention window\n        causal: Whether to use causal masking\n    \"\"\"\n    batch_size, seq_len, d_k = Q.shape\n    \n    # Compute attention scores\n    scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)\n    \n    # Create sliding window mask\n    mask = torch.zeros(seq_len, seq_len, dtype=torch.bool)\n    for i in range(seq_len):\n        start = max(0, i - window_size // 2)\n        end = min(seq_len, i + window_size // 2 + 1)\n        mask[i, start:end] = True\n        \n        if causal:\n            mask[i, i+1:] = False\n    \n    # Apply mask\n    scores = scores.masked_fill(~mask.unsqueeze(0), float('-inf'))\n    attn_weights = F.softmax(scores, dim=-1)\n    output = torch.matmul(attn_weights, V)\n    \n    return output, attn_weights\n\n# Test sliding window attention\nseq_len = 2048\nd_k = 64\nwindow_size = 512\n\nQ = torch.randn(1, seq_len, d_k)\nK = torch.randn(1, seq_len, d_k)\nV = torch.randn(1, seq_len, d_k)\n\noutput, attn = sliding_window_attention(Q, K, V, window_size)\n\nprint(f\"Sequence length: {seq_len}\")\nprint(f\"Window size: {window_size}\")\nprint(f\"Output shape: {output.shape}\")\nprint(f\"Memory efficient: O(seq_len \u00d7 window_size) instead of O(seq_len\u00b2)\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n\n\u2705 **Module Complete**\n\n## Next Steps\n\nContinue to the next module in the course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}