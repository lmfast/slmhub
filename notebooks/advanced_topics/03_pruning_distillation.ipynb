{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4.3: Pruning & Distillation\n\n**Goal**: Compress models through pruning and knowledge distillation\n\n**Time**: 90 minutes\n\n**Concepts Covered**:\n- Magnitude pruning implementation\n- Structured pruning (heads, neurons)\n- Knowledge distillation with temperature\n- Distill Phi-3-Mini \u2192 SmolLM-1.7B\n- Compare pruned vs distilled models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install torch transformers accelerate matplotlib seaborn numpy -q"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef magnitude_pruning(model, sparsity=0.5):\n    \"\"\"Magnitude-based unstructured pruning\"\"\"\n    pruned_model = model\n    for name, param in pruned_model.named_parameters():\n        if 'weight' in name and len(param.shape) >= 2:\n            # Calculate threshold\n            flat_param = param.data.abs().flatten()\n            threshold = torch.quantile(flat_param, sparsity)\n            # Create mask\n            mask = param.data.abs() > threshold\n            param.data *= mask.float()\n    return pruned_model\n\ndef knowledge_distillation_loss(student_logits, teacher_logits, temperature=3.0, alpha=0.7):\n    \"\"\"Knowledge distillation loss with temperature scaling\"\"\"\n    # Soft targets from teacher\n    teacher_probs = F.softmax(teacher_logits / temperature, dim=-1)\n    student_log_probs = F.log_softmax(student_logits / temperature, dim=-1)\n    \n    # KL divergence\n    kd_loss = F.kl_div(student_log_probs, teacher_probs, reduction='batchmean') * (temperature ** 2)\n    \n    # Hard targets (ground truth)\n    # hard_loss = F.cross_entropy(student_logits, labels)\n    \n    # Combined\n    # total_loss = alpha * kd_loss + (1 - alpha) * hard_loss\n    \n    return kd_loss\n\nprint(\"Pruning removes less important weights\")\nprint(\"Distillation transfers knowledge from teacher to student model\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n\n\u2705 **Module Complete**\n\n## Next Steps\n\nContinue to the next module in the course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}