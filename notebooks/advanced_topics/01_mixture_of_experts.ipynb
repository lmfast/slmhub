{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4.1: Mixture of Experts (MoE)\n\n**Goal**: Implement MoE layer from scratch and understand routing mechanisms\n\n**Time**: 90 minutes\n\n**Concepts Covered**:\n- MoE layer implementation\n- Router mechanism with top-k selection\n- Compare dense vs MoE memory/compute\n- Convert existing model to MoE\n- Visualize expert routing patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install torch transformers accelerate matplotlib seaborn numpy -q"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nclass MoELayer(nn.Module):\n    \"\"\"Mixture of Experts Layer with Top-K Routing\"\"\"\n    def __init__(self, d_model, num_experts=8, top_k=2, expert_capacity_factor=1.25):\n        super().__init__()\n        self.num_experts = num_experts\n        self.top_k = top_k\n        self.d_model = d_model\n        \n        # Router: maps input to expert scores\n        self.router = nn.Linear(d_model, num_experts)\n        \n        # Expert networks (simple FFNs)\n        self.experts = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(d_model, d_model * 4),\n                nn.GELU(),\n                nn.Linear(d_model * 4, d_model)\n            ) for _ in range(num_experts)\n        ])\n        \n    def forward(self, x):\n        batch_size, seq_len, d_model = x.shape\n        \n        # Router scores\n        router_logits = self.router(x)  # (batch, seq_len, num_experts)\n        router_probs = F.softmax(router_logits, dim=-1)\n        \n        # Top-k selection\n        top_k_probs, top_k_indices = torch.topk(router_probs, self.top_k, dim=-1)\n        top_k_probs = top_k_probs / top_k_probs.sum(dim=-1, keepdim=True)  # Renormalize\n        \n        # Process through experts\n        output = torch.zeros_like(x)\n        for expert_idx in range(self.num_experts):\n            # Find positions routed to this expert\n            expert_mask = (top_k_indices == expert_idx)\n            if expert_mask.any():\n                expert_input = x[expert_mask]\n                expert_output = self.experts[expert_idx](expert_input)\n                # Weight by routing probability\n                expert_weights = top_k_probs[expert_mask]\n                output[expert_mask] += expert_output * expert_weights.unsqueeze(-1)\n        \n        return output, router_probs\n\n# Test MoE layer\nd_model = 128\nnum_experts = 4\ntop_k = 2\nmoe = MoELayer(d_model, num_experts, top_k)\n\nx = torch.randn(2, 10, d_model)\noutput, routing = moe(x)\n\nprint(f\"Input shape: {x.shape}\")\nprint(f\"Output shape: {output.shape}\")\nprint(f\"Routing probabilities shape: {routing.shape}\")\nprint(f\"\\nTop-{top_k} routing active for each token\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n\n\u2705 **Module Complete**\n\n## Next Steps\n\nContinue to the next module in the course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}