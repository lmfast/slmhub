{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 12.2: Personal Knowledge Base (RAG)\n\n**Goal**: Build a complete RAG system for personal knowledge management\n\n**Time**: 120 minutes\n\n**Concepts Covered**:\n- Document loading and chunking\n- Embedding generation\n- FAISS vector store setup\n- Multi-hop RAG implementation\n- Query interface\n- Performance optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install torch transformers accelerate matplotlib seaborn numpy -q"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Personal Knowledge Base RAG System\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import HuggingFaceEmbeddings\nimport faiss\nimport numpy as np\n\nclass PersonalKnowledgeBase:\n    def __init__(self, embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"):\n        self.text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=500,\n            chunk_overlap=50,\n        )\n        self.embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n        self.vector_store = None\n        self.documents = []\n    \n    def add_documents(self, texts):\n        \"\"\"Add documents to knowledge base\"\"\"\n        # Chunk documents\n        chunks = []\n        for text in texts:\n            chunks.extend(self.text_splitter.split_text(text))\n        \n        # Generate embeddings\n        chunk_embeddings = self.embeddings.embed_documents(chunks)\n        \n        # Create FAISS index\n        dimension = len(chunk_embeddings[0])\n        index = faiss.IndexFlatL2(dimension)\n        \n        # Add embeddings\n        embeddings_array = np.array(chunk_embeddings).astype('float32')\n        index.add(embeddings_array)\n        \n        self.vector_store = index\n        self.documents = chunks\n        \n        print(f\"Added {len(chunks)} chunks to knowledge base\")\n    \n    def search(self, query, top_k=5):\n        \"\"\"Search knowledge base\"\"\"\n        # Embed query\n        query_embedding = self.embeddings.embed_query(query)\n        query_vector = np.array([query_embedding]).astype('float32')\n        \n        # Search\n        distances, indices = self.vector_store.search(query_vector, top_k)\n        \n        # Retrieve documents\n        results = [self.documents[idx] for idx in indices[0]]\n        \n        return results\n\n# Example usage\nkb = PersonalKnowledgeBase()\ndocuments = [\n    \"Python is a programming language...\",\n    \"Machine learning uses algorithms...\",\n    # Add more documents\n]\n\n# kb.add_documents(documents)\n# results = kb.search(\"What is Python?\")\n\nprint(\"Personal Knowledge Base:\")\nprint(\"- Document chunking\")\nprint(\"- Embedding generation\")\nprint(\"- FAISS vector search\")\nprint(\"- Query interface\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Multi-Hop RAG\nclass MultiHopRAG:\n    def __init__(self, knowledge_base, llm):\n        self.kb = knowledge_base\n        self.llm = llm\n    \n    def query(self, question, max_hops=3):\n        \"\"\"Multi-hop reasoning with RAG\"\"\"\n        context = []\n        current_query = question\n        \n        for hop in range(max_hops):\n            # Retrieve relevant documents\n            docs = self.kb.search(current_query, top_k=3)\n            context.extend(docs)\n            \n            # Generate refined query or answer\n            if hop < max_hops - 1:\n                # Refine query for next hop\n                refinement_prompt = f\"\"\"\nQuestion: {question}\nContext so far: {''.join(context[-3:])}\nWhat additional information do we need to answer this question?\n\"\"\"\n                current_query = self.llm.generate(refinement_prompt)\n            else:\n                # Final answer\n                answer_prompt = f\"\"\"\nQuestion: {question}\nContext: {''.join(context)}\nAnswer:\n\"\"\"\n                answer = self.llm.generate(answer_prompt)\n                return answer\n        \n        return \"Unable to answer\"\n\nprint(\"Multi-Hop RAG:\")\nprint(\"- Iterative retrieval\")\nprint(\"- Query refinement\")\nprint(\"- Better for complex questions\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n\n\u2705 **Module Complete**\n\n## Next Steps\n\nContinue to the next module in the course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}