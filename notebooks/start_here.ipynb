{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Start Here: The Developer's Path\n",
                "\n",
                "Welcome to SLM Hub. This notebook serves as an interactive introduction to Small Language Models (SLMs). You can run the code cells below to check your environment's readiness for SLMs."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## What are Small Language Models?\n",
                "\n",
                "**Small Language Models (SLMs)** are a class of language models designed to be efficient, accessible, and practical for real-world deployment. Unlike their larger counterparts (LLMs), SLMs prioritize resource efficiency while maintaining strong performance on targeted tasks."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Environment Check\n",
                "Let's see what hardware you have available for running SLMs. This simple script checks for GPU availability (CUDA/MPS)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "\n",
                "def check_hardware():\n",
                "    if torch.cuda.is_available():\n",
                "        return f\"✅ GPU Detected: {torch.cuda.get_device_name(0)} (VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB)\"\n",
                "    elif torch.backends.mps.is_available():\n",
                "        return \"✅ Apple Silicon GPU (MPS) Detected\"\n",
                "    else:\n",
                "        return \"⚠️ No GPU detected. You can still run SLMs on CPU (slower but functional).\"\n",
                "\n",
                "print(check_hardware())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Defining Characteristics of SLMs\n",
                "\n",
                "### 1. **Resource Efficiency**\n",
                "SLMs run on constrained hardware:\n",
                "- Consumer-grade GPUs (4-16GB VRAM)\n",
                "- CPUs with 8-32GB RAM\n",
                "- Edge devices (Phones, IoT)\n",
                "\n",
                "### 2. **Fast Inference**\n",
                "Low latency makes them ideal for interactive chatbots and detailed analysis."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Quick Demo: Loading a Tiny Model\n",
                "Let's load a tiny SLM (`gpt2` or similar small proxy for this demo) just to prove you can run inference right now."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install transformers accelerate"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import pipeline\n",
                "\n",
                "# Ideally we'd use Phi-3, but for a quick 'Start Here' demo without auth, we use a public tiny model\n",
                "generator = pipeline('text-generation', model='gpt2')\n",
                "print(generator(\"Hello, I am a small language model\", max_length=30, num_return_sequences=1)[0]['generated_text'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Next Steps\n",
                "- **Deploy Locally**: Try the [Ollama Quickstart](/slmhub/docs/deploy/quickstarts/ollama/)\n",
                "- **Learn Fundamentals**: Read [SLM vs LLM](/slmhub/docs/learn/fundamentals/slm-vs-llm/)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}