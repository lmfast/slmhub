{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# vLLM Quickstart: High-Throughput Serving\n",
                "\n",
                "This notebook demonstrates how to serve an SLM using vLLM, a high-throughput and memory-efficient inference engine. Guide: [vLLM Deployment](https://slmhub.gitbook.io/slmhub/docs/deploy/quickstarts/vllm).\n",
                "\n",
                "## 1. Install vLLM\n",
                "vLLM requires a GPU (T4 is supported)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install vllm"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Offline Inference\n",
                "Load the model and generate text directly in Python."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from vllm import LLM, SamplingParams\n",
                "\n",
                "# Initialize model (Phi-3-mini is small and fast)\n",
                "llm = LLM(model=\"microsoft/Phi-3-mini-4k-instruct\", trust_remote_code=True)\n",
                "\n",
                "# Define prompts\n",
                "prompts = [\n",
                "    \"Hello, my name is\",\n",
                "    \"The future of AI is\",\n",
                "    \"Write a short poem about coding.\"\n",
                "]\n",
                "\n",
                "# Sampling parameters\n",
                "sampling_params = SamplingParams(temperature=0.7, top_p=0.95, max_tokens=60)\n",
                "\n",
                "# Generate\n",
                "outputs = llm.generate(prompts, sampling_params)\n",
                "\n",
                "# Print results\n",
                "for output in outputs:\n",
                "    prompt = output.prompt\n",
                "    generated_text = output.outputs[0].text\n",
                "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Server Mode (OpenAI Compatible)\n",
                "Run vLLM as an API server. (Note: In Colab, this blocks the cell. We run it in background)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Start server in background\n",
                "import subprocess\n",
                "import time\n",
                "\n",
                "command = [\n",
                "    \"python\", \"-m\", \"vllm.entrypoints.openai.api_server\",\n",
                "    \"--model\", \"microsoft/Phi-3-mini-4k-instruct\",\n",
                "    \"--trust-remote-code\",\n",
                "    \"--dtype\", \"auto\",\n",
                "    \"--port\", \"8000\"\n",
                "]\n",
                "\n",
                "process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
                "print(\"Starting vLLM server... (takes ~1-2 mins to load model)\")\n",
                "time.sleep(60) # Wait for model load"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Query the API\n",
                "Use standard OpenAI client to query the local vLLM server."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install openai"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from openai import OpenAI\n",
                "\n",
                "client = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"vllm\")\n",
                "\n",
                "try:\n",
                "    completion = client.chat.completions.create(\n",
                "        model=\"microsoft/Phi-3-mini-4k-instruct\",\n",
                "        messages=[\n",
                "            {\"role\": \"user\", \"content\": \"Explain vLLM in one sentence.\"}\n",
                "        ]\n",
                "    )\n",
                "    print(\"Response:\", completion.choices[0].message.content)\n",
                "except Exception as e:\n",
                "    print(\"Server might still be loading or failed:\", e)\n",
                "    # Print logs if failed\n",
                "    out, err = process.communicate(timeout=1)\n",
                "    print(err.decode())"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}