{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 13.1: Resources & Quick Reference\n\n**Goal**: Comprehensive resource compilation and quick reference guides\n\n**Time**: 30 minutes\n\n**Concepts Covered**:\n- Official documentation links\n- Must-read papers list\n- Community resources\n- Model selection flowchart\n- Optimization techniques summary\n- Quick reference tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install torch transformers accelerate matplotlib seaborn numpy -q"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Resources & Quick Reference\n\n# Official Documentation\ndocumentation = {\n    \"Transformers\": \"https://huggingface.co/docs/transformers\",\n    \"vLLM\": \"https://docs.vllm.ai\",\n    \"PEFT\": \"https://huggingface.co/docs/peft\",\n    \"TRL\": \"https://huggingface.co/docs/trl\",\n    \"PyTorch\": \"https://pytorch.org/docs\",\n}\n\n# Must-Read Papers\npapers = {\n    \"Attention Is All You Need\": \"https://arxiv.org/abs/1706.03762\",\n    \"LoRA: Low-Rank Adaptation\": \"https://arxiv.org/abs/2106.09685\",\n    \"Chinchilla Scaling Laws\": \"https://arxiv.org/abs/2203.15556\",\n    \"Mamba: Linear-Time Sequence Modeling\": \"https://arxiv.org/abs/2312.00752\",\n    \"BitNet: Scaling 1-bit Transformers\": \"https://arxiv.org/abs/2310.11453\",\n    \"Constitutional AI\": \"https://arxiv.org/abs/2212.08073\",\n}\n\n# Community Resources\ncommunity = {\n    \"Discord\": \"https://discord.gg/slmhub\",\n    \"GitHub\": \"https://github.com/lmfast/slmhub\",\n    \"HuggingFace\": \"https://huggingface.co\",\n    \"Papers with Code\": \"https://paperswithcode.com\",\n}\n\nprint(\"Resources:\")\nprint(\"\\nDocumentation:\")\nfor name, url in documentation.items():\n    print(f\"  {name}: {url}\")\n\nprint(\"\\nMust-Read Papers:\")\nfor name, url in papers.items():\n    print(f\"  {name}: {url}\")\n\nprint(\"\\nCommunity:\")\nfor name, url in community.items():\n    print(f\"  {name}: {url}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Model Selection Flowchart\ndef select_model(use_case, memory_gb, latency_ms=None, accuracy_requirement=\"medium\"):\n    \"\"\"Model selection helper\"\"\"\n    recommendations = []\n    \n    if memory_gb < 1:\n        recommendations.append(\"SmolLM-135M (INT4)\")\n    elif memory_gb < 2:\n        recommendations.append(\"SmolLM-360M (INT4)\")\n    elif memory_gb < 4:\n        recommendations.append(\"SmolLM-1.7B (INT4)\")\n    elif memory_gb < 8:\n        recommendations.append(\"SmolLM-1.7B (FP16) or Phi-3-mini (INT4)\")\n    else:\n        recommendations.append(\"Phi-3-mini (FP16) or larger models\")\n    \n    if use_case == \"code\":\n        recommendations.append(\"Consider: StarCoder2, CodeQwen\")\n    elif use_case == \"math\":\n        recommendations.append(\"Consider: DeepSeek-Math\")\n    elif use_case == \"multilingual\":\n        recommendations.append(\"Consider: Qwen2.5, XGLM\")\n    \n    return recommendations\n\nprint(\"Model Selection Guide:\")\nprint(select_model(\"general\", memory_gb=4, accuracy_requirement=\"high\"))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Quick Reference Tables\n\n# Quantization Comparison\nquantization_table = {\n    \"FP32\": {\"bits\": 32, \"memory_reduction\": \"1x\", \"quality\": \"100%\", \"speed\": \"1x\"},\n    \"FP16\": {\"bits\": 16, \"memory_reduction\": \"2x\", \"quality\": \"99%\", \"speed\": \"1.5x\"},\n    \"INT8\": {\"bits\": 8, \"memory_reduction\": \"4x\", \"quality\": \"95%\", \"speed\": \"2x\"},\n    \"INT4\": {\"bits\": 4, \"memory_reduction\": \"8x\", \"quality\": \"90%\", \"speed\": \"3x\"},\n    \"BitNet\": {\"bits\": 1.58, \"memory_reduction\": \"16x\", \"quality\": \"85%\", \"speed\": \"4x\"},\n}\n\nprint(\"Quantization Comparison:\")\nfor method, specs in quantization_table.items():\n    print(f\"  {method}: {specs['bits']} bits, {specs['memory_reduction']} memory, {specs['quality']} quality\")\n\n# Optimization Techniques\noptimization_techniques = {\n    \"LoRA\": \"Parameter-efficient fine-tuning\",\n    \"QLoRA\": \"LoRA with 4-bit quantization\",\n    \"Gradient Checkpointing\": \"Trade compute for memory\",\n    \"Mixed Precision\": \"FP16/BF16 training\",\n    \"FlashAttention\": \"Memory-efficient attention\",\n    \"KV Cache\": \"Faster generation\",\n    \"Speculative Decoding\": \"2-3x speedup\",\n}\n\nprint(\"\\nOptimization Techniques:\")\nfor technique, description in optimization_techniques.items():\n    print(f\"  {technique}: {description}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n\n\u2705 **Module Complete**\n\n## Next Steps\n\nContinue to the next module in the course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}