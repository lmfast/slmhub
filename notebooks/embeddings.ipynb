{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Embeddings & Semantic Search\n",
                "\n",
                "This notebook covers how to use embeddings for semantic search, corresponding to the SLM Hub [Embeddings Guide](https://slmhub.gitbook.io/slmhub/docs/learn/fundamentals/embeddings).\n",
                "\n",
                "## 1. Setup\n",
                "Install `sentence-transformers` for embeddings and `chromadb` for the vector store."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install sentence-transformers chromadb scikit-learn"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Basic Embeddings\n",
                "We use `sentence-transformers` to convert text into vectors."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sentence_transformers import SentenceTransformer\n",
                "\n",
                "# Load a small, fast embedding model\n",
                "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
                "\n",
                "# Create embeddings\n",
                "texts = [\n",
                "    \"Small language models are efficient\",\n",
                "    \"SLMs use less compute than LLMs\",\n",
                "    \"The weather is nice today\"\n",
                "]\n",
                "\n",
                "embeddings = model.encode(texts)\n",
                "print(f\"Embeddings shape: {embeddings.shape}\")  # (3, 384)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Calculate Similarity\n",
                "We can use Cosine Similarity to see how related the texts are."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics.pairwise import cosine_similarity\n",
                "\n",
                "similarities = cosine_similarity(embeddings)\n",
                "\n",
                "print(\"Similarity Matrix:\")\n",
                "print(similarities)\n",
                "\n",
                "# Text 0 (\"Small language...\") vs Text 1 (\"SLMs use less...\") should be high\n",
                "print(f\"\\nSimilarity (0 vs 1): {similarities[0][1]:.4f}\")\n",
                "# Text 0 vs Text 2 (\"Weather...\") should be low\n",
                "print(f\"Similarity (0 vs 2): {similarities[0][2]:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. RAG Example with ChromaDB\n",
                "A simple example of Retrieval Augmented Generation logic: Indexing documents and searching by query."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import chromadb\n",
                "\n",
                "# Initialize Chroma Client\n",
                "client = chromadb.Client()\n",
                "# Create collection (delete if exists to reset)\n",
                "try:\n",
                "    client.delete_collection(\"docs\")\n",
                "except:\n",
                "    pass\n",
                "collection = client.create_collection(\"docs\")\n",
                "\n",
                "# Documents to index\n",
                "documents = [\n",
                "    \"SLMs are models with fewer parameters, typically under 10B.\",\n",
                "    \"Fine-tuning adapts models to specific tasks using data.\",\n",
                "    \"Quantization reduces model size by lowering precision (e.g., 4-bit).\"\n",
                "]\n",
                "\n",
                "# Generate embeddings for docs\n",
                "doc_embeddings = model.encode(documents)\n",
                "\n",
                "# Add to Chroma\n",
                "collection.add(\n",
                "    embeddings=doc_embeddings.tolist(),\n",
                "    documents=documents,\n",
                "    ids=[\"doc1\", \"doc2\", \"doc3\"]\n",
                ")\n",
                "\n",
                "# Perform a Search\n",
                "query = \"How to make models smaller?\"\n",
                "query_embed = model.encode(query)\n",
                "\n",
                "results = collection.query(\n",
                "    query_embeddings=[query_embed.tolist()],\n",
                "    n_results=2\n",
                ")\n",
                "\n",
                "print(f\"Query: '{query}'\")\n",
                "print(\"\\nTop Results:\")\n",
                "for doc in results[\"documents\"][0]:\n",
                "    print(f\" - {doc}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}