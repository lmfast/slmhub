{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Generative AI & Small Language Models (SLMs)\n",
                "\n",
                "Understanding the basics of GenAI and where SLMs fit into the hierarchy. Guide: [GenAI Basics](/slmhub/docs/learn/fundamentals/genai-basics).\n",
                "\n",
                "## 1. The AI Hierarchy\n",
                "Where do SLMs fit? Let's visualize."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\"\"\n",
                "AI (Artificial Intelligence)\n",
                "├── Machine Learning (ML)\n",
                "│   ├── Deep Learning (DL)\n",
                "│   │   ├── Generative AI (GenAI)\n",
                "│   │   │   ├── LLMs (GPT-4, Claude) -> Cloud, Huge, $$$$\n",
                "│   │   │   └── SLMs (Phi-3, Llama-3-8B) -> Local, Fast, Free <--- YOU ARE HERE\n",
                "\"\"\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Token Probability Demo\n",
                "SLMs are just 'next token predictors'. Let's see this in action using a small model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install transformers torch accelerate"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
                "import torch\n",
                "\n",
                "# Load a very small model for demonstration (GPT-2 is small/fast)\n",
                "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
                "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
                "\n",
                "text = \"The sky is\"\n",
                "inputs = tokenizer(text, return_tensors=\"pt\")\n",
                "\n",
                "with torch.no_grad():\n",
                "    outputs = model(**inputs)\n",
                "    predictions = outputs.logits[0, -1, :]\n",
                "    probs = torch.softmax(predictions, dim=-1)\n",
                "\n",
                "# Get top 5 predictions\n",
                "top_k = 5\n",
                "top_probs, top_indices = torch.topk(probs, top_k)\n",
                "\n",
                "print(f\"Input: '{text}' ... Predicting next word:\\n\")\n",
                "for i in range(top_k):\n",
                "    word = tokenizer.decode(top_indices[i])\n",
                "    prob = top_probs[i].item()\n",
                "    print(f\"{word.strip():<10} : {prob:.2%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Temperature\n",
                "Temperature controls randomness. Low temp = focused. High temp = creative."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate(text, temp):\n",
                "    print(f\"--- Temperature: {temp} ---\")\n",
                "    out = model.generate(**tokenizer(text, return_tensors=\"pt\"), \n",
                "                         max_new_tokens=20, \n",
                "                         temperature=temp, \n",
                "                         do_sample=True, \n",
                "                         top_k=50)\n",
                "    print(tokenizer.decode(out[0], skip_special_tokens=True) + \"\\n\")\n",
                "\n",
                "text = \"The confusing thing about time travel is\"\n",
                "generate(text, 0.1) # Focused\n",
                "generate(text, 1.5) # Crazy"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}