{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Ollama Quickstart on Google Colab\n",
                "\n",
                "This notebook demonstrates how to install and run [Ollama](https://ollama.com/) directly in a Google Colab environment (Free Tier compatible).\n",
                "\n",
                "## 1. Install Ollama\n",
                "We will download and install the Ollama Linux binary."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!curl -fsSL https://ollama.com/install.sh | sh"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Start the Ollama Server\n",
                "Ollama needs to run in the background to serve requests. We use Python's `subprocess` to start it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import subprocess\n",
                "import time\n",
                "\n",
                "# Start Ollama serve in the background\n",
                "process = subprocess.Popen([\"ollama\", \"serve\"])\n",
                "\n",
                "# Give it a few seconds to initialize\n",
                "time.sleep(5)\n",
                "print(\"Ollama server started!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Run a Model\n",
                "Now we can pull and run a model. Let's try `phi4`, a powerful SLM from Microsoft.\n",
                "\n",
                "> **Note:** The first run will take some time to download the model weights (approx 2-3GB)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!ollama run phi4 \"Why are Small Language Models (SLMs) important in 2026?\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Use via API\n",
                "You can also query the model using curl or python libraries, as Ollama provides an OpenAI-compatible API."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!curl http://localhost:11434/v1/chat/completions \\\n",
                "  -H \"Content-Type: application/json\" \\\n",
                "  -d '{\n",
                "    \"model\": \"phi4\",\n",
                "    \"messages\": [{\"role\": \"user\", \"content\": \"Explain quantum computing in one sentence.\"}]\n",
                "  }'"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}