{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# llama.cpp Quickstart (Python)\n",
                "\n",
                "This notebook demonstrates how to run quantized GGUF models using `llama-cpp-python`. Guide: [llama.cpp Deployment](https://slmhub.gitbook.io/slmhub/docs/deploy/quickstarts/llama-cpp).\n",
                "\n",
                "## 1. Install\n",
                "We install `llama-cpp-python` with CUDA support (if GPU available) for acceleration."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!CMAKE_ARGS=\"-DGGML_CUDA=on\" pip install llama-cpp-python"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Download a GGUF Model\n",
                "We use `huggingface_hub` to download a pre-quantized GGUF of Phi-3."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from huggingface_hub import hf_hub_download\n",
                "\n",
                "model_name = \"microsoft/Phi-3-mini-4k-instruct-gguf\"\n",
                "model_file = \"Phi-3-mini-4k-instruct-q4.gguf\"\n",
                "\n",
                "model_path = hf_hub_download(model_name, filename=model_file)\n",
                "print(f\"Model downloaded to: {model_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Run Inference\n",
                "Load the model and generate text."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from llama_cpp import Llama\n",
                "\n",
                "# Initialize\n",
                "llm = Llama(\n",
                "    model_path=model_path,\n",
                "    n_gpu_layers=-1, # Offload all layers to GPU\n",
                "    n_ctx=2048,\n",
                "    verbose=False\n",
                ")\n",
                "\n",
                "# Generate\n",
                "output = llm(\n",
                "    \"Q: Name the planets in the solar system. A: \", \n",
                "    max_tokens=64, \n",
                "    stop=[\"Q:\", \"\\n\"],\n",
                "    echo=True\n",
                ")\n",
                "\n",
                "print(output['choices'][0]['text'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Chat Format\n",
                "Using the chat API which handles special tokens."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "response = llm.create_chat_completion(\n",
                "    messages=[\n",
                "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
                "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
                "    ]\n",
                ")\n",
                "\n",
                "print(response['choices'][0]['message']['content'])"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}