{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 9.4: Hybrid Architectures\n\n**Goal**: Build Jamba-style hybrid models\n\n**Time**: 90 minutes\n\n**Concepts Covered**:\n- Jamba-style hybrid (Mamba + Attention)\n- Layer placement strategies\n- Attention frequency experiments\n- Quality vs speed trade-offs\n- Custom hybrid builder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install torch transformers accelerate matplotlib seaborn numpy -q"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\nimport torch.nn as nn\n\nclass HybridBlock(nn.Module):\n    \"\"\"Hybrid Mamba + Attention block (Jamba-style)\"\"\"\n    def __init__(self, d_model, num_heads=8, mamba_expand=2):\n        super().__init__()\n        self.d_model = d_model\n        \n        # Mamba layer\n        from advanced_architectures.mamba import SelectiveSSM\n        self.mamba = SelectiveSSM(d_model)\n        \n        # Attention layer\n        self.attention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n        \n        # Layer norms\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        \n        # FFN\n        self.ffn = nn.Sequential(\n            nn.Linear(d_model, d_model * 4),\n            nn.GELU(),\n            nn.Linear(d_model * 4, d_model)\n        )\n        \n    def forward(self, x, use_attention=True):\n        # Mamba branch\n        mamba_out = self.mamba(x)\n        x = self.norm1(x + mamba_out)\n        \n        # Optional attention branch\n        if use_attention:\n            attn_out, _ = self.attention(x, x, x)\n            x = self.norm2(x + attn_out)\n        \n        # FFN\n        ffn_out = self.ffn(x)\n        x = self.norm3(x + ffn_out)\n        \n        return x\n\ndef create_hybrid_model(num_layers, attention_frequency=4):\n    \"\"\"Create hybrid model with attention every N layers\"\"\"\n    layers = []\n    for i in range(num_layers):\n        use_attention = (i % attention_frequency == 0)\n        layers.append(HybridBlock(d_model=512, use_attention=use_attention))\n    return nn.Sequential(*layers)\n\nprint(\"Hybrid Architecture Benefits:\")\nprint(\"- Mamba: Efficient long-context (O(n))\")\nprint(\"- Attention: Strong short-range dependencies\")\nprint(\"- Best of both worlds: Quality + Speed\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n\n\u2705 **Module Complete**\n\n## Next Steps\n\nContinue to the next module in the course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}