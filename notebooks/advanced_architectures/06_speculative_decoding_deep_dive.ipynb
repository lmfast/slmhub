{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 9.6: Speculative Decoding Deep Dive\n\n**Goal**: Implement and optimize speculative decoding\n\n**Time**: 90 minutes\n\n**Concepts Covered**:\n- Draft + target model setup\n- Parallel verification\n- Acceptance/rejection logic\n- Speedup measurement\n- Optimal k selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install torch transformers accelerate matplotlib seaborn numpy -q"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\nimport torch.nn.functional as F\n\nclass SpeculativeDecoder:\n    \"\"\"Speculative decoding for faster generation\"\"\"\n    def __init__(self, draft_model, target_model):\n        self.draft_model = draft_model  # Small, fast model\n        self.target_model = target_model  # Large, accurate model\n    \n    def generate_draft(self, input_ids, k=5):\n        \"\"\"Generate k tokens with draft model\"\"\"\n        draft_tokens = []\n        current_ids = input_ids\n        \n        for _ in range(k):\n            logits = self.draft_model(current_ids).logits[:, -1, :]\n            next_token = torch.argmax(logits, dim=-1)\n            draft_tokens.append(next_token)\n            current_ids = torch.cat([current_ids, next_token.unsqueeze(1)], dim=1)\n        \n        return torch.stack(draft_tokens, dim=1)\n    \n    def verify_draft(self, input_ids, draft_tokens):\n        \"\"\"Verify draft tokens with target model\"\"\"\n        # Run target model on input + draft tokens\n        full_sequence = torch.cat([input_ids, draft_tokens], dim=1)\n        target_logits = self.target_model(full_sequence).logits\n        \n        # Acceptance probability for each draft token\n        accepted_tokens = []\n        current_ids = input_ids\n        \n        for i, draft_token in enumerate(draft_tokens.unbind(1)):\n            # Get target model probability for draft token\n            target_probs = F.softmax(target_logits[:, input_ids.size(1) + i - 1, :], dim=-1)\n            draft_prob = target_probs.gather(1, draft_token.unsqueeze(1))\n            \n            # Get draft model probability\n            draft_logits = self.draft_model(current_ids).logits[:, -1, :]\n            draft_model_prob = F.softmax(draft_logits, dim=-1).gather(1, draft_token.unsqueeze(1))\n            \n            # Acceptance probability\n            accept_prob = torch.min(torch.ones_like(draft_prob), target_probs / (draft_model_prob + 1e-10))\n            \n            # Accept or reject\n            if torch.rand(1) < accept_prob:\n                accepted_tokens.append(draft_token)\n                current_ids = torch.cat([current_ids, draft_token.unsqueeze(1)], dim=1)\n            else:\n                # Sample from adjusted distribution\n                adjusted_probs = F.normalize(torch.clamp(target_probs - draft_model_prob, min=0), p=1, dim=1)\n                new_token = torch.multinomial(adjusted_probs, 1)\n                accepted_tokens.append(new_token.squeeze(1))\n                break\n        \n        return torch.stack(accepted_tokens, dim=1) if accepted_tokens else None\n\nprint(\"Speculative Decoding:\")\nprint(\"- Draft model: Fast, generates k tokens\")\nprint(\"- Target model: Accurate, verifies draft\")\nprint(\"- Speedup: 2-3x for compatible models\")\nprint(\"- Optimal k: Usually 3-5 tokens\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n\n\u2705 **Module Complete**\n\n## Next Steps\n\nContinue to the next module in the course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}