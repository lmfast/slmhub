{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 9.7: Quantization Theory\n\n**Goal**: Deep dive into quantization mathematics\n\n**Time**: 90 minutes\n\n**Concepts Covered**:\n- Uniform quantization math\n- Per-tensor vs per-channel\n- GPTQ algorithm implementation\n- Quantization error analysis\n- Weight distribution visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install torch transformers accelerate matplotlib seaborn numpy -q"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef uniform_quantize(weights, bits=8):\n    \"\"\"Uniform quantization\"\"\"\n    # Calculate scale and zero point\n    w_min = weights.min()\n    w_max = weights.max()\n    \n    scale = (w_max - w_min) / (2 ** bits - 1)\n    zero_point = -w_min / scale\n    \n    # Quantize\n    q_weights = torch.round(weights / scale + zero_point)\n    q_weights = torch.clamp(q_weights, 0, 2 ** bits - 1)\n    \n    # Dequantize\n    dequantized = (q_weights - zero_point) * scale\n    \n    return dequantized, scale, zero_point\n\ndef gptq_quantize(layer, bits=4):\n    \"\"\"GPTQ: Optimal quantization with Hessian\"\"\"\n    # Simplified GPTQ algorithm\n    weights = layer.weight.data.clone()\n    num_bits = bits\n    \n    # Per-channel quantization\n    scales = []\n    quantized_weights = []\n    \n    for channel in range(weights.shape[0]):\n        channel_weights = weights[channel]\n        \n        # Find optimal scale for this channel\n        w_abs_max = channel_weights.abs().max()\n        scale = w_abs_max / (2 ** (num_bits - 1) - 1)\n        \n        # Quantize\n        q_weights = torch.round(channel_weights / scale)\n        q_weights = torch.clamp(q_weights, -2 ** (num_bits - 1), 2 ** (num_bits - 1) - 1)\n        \n        # Dequantize\n        dequantized = q_weights * scale\n        \n        scales.append(scale)\n        quantized_weights.append(dequantized)\n    \n    quantized_weights = torch.stack(quantized_weights)\n    \n    return quantized_weights, scales\n\n# Example\nweights = torch.randn(128, 256) * 0.1\n\n# Uniform quantization\nuniform_q, scale, zp = uniform_quantize(weights, bits=8)\nuniform_error = (weights - uniform_q).abs().mean()\n\n# GPTQ quantization\ngptq_q, scales = gptq_quantize(torch.nn.Linear(256, 128), bits=4)\ngptq_error = (weights - gptq_q).abs().mean()\n\nprint(f\"Uniform quantization error: {uniform_error:.6f}\")\nprint(f\"GPTQ quantization error: {gptq_error:.6f}\")\nprint(\"\\nGPTQ uses per-channel quantization for better accuracy\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n\n\u2705 **Module Complete**\n\n## Next Steps\n\nContinue to the next module in the course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}