{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1.3: Feed-Forward Networks & Normalization\n\n**Goal**: Understand FFN architectures and normalization methods\n\n**Time**: 45 minutes\n\n**Concepts Covered**:\n- SwiGLU activation function\n- ReLU vs GELU vs SwiGLU comparison\n- LayerNorm vs RMSNorm\n- Memory profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install torch numpy matplotlib seaborn -q"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\n\ntorch.manual_seed(42)\nnp.random.seed(42)\nplt.style.use('seaborn-v0_8-darkgrid')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 1: Activation Functions (15 mins)\n\nCompare ReLU, GELU, and SwiGLU activation functions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define activation functions\ndef relu(x):\n    return F.relu(x)\n\ndef gelu(x):\n    return F.gelu(x)\n\ndef swiglu(x):\n    \"\"\"SwiGLU: Swish-Gated Linear Unit\n    SwiGLU(x) = Swish(xW + b) \u2299 (xV + c)\n    where Swish(x) = x * sigmoid(x)\n    \"\"\"\n    x1, x2 = x.chunk(2, dim=-1)\n    return F.silu(x1) * x2  # SiLU is Swish\n\n# Visualize activations\nx = torch.linspace(-5, 5, 1000)\ny_relu = relu(x)\ny_gelu = gelu(x)\n# For SwiGLU, we need two inputs, so we'll show Swish\ny_swish = F.silu(x)\n\nplt.figure(figsize=(12, 5))\nplt.plot(x.numpy(), y_relu.numpy(), label='ReLU', linewidth=2)\nplt.plot(x.numpy(), y_gelu.numpy(), label='GELU', linewidth=2)\nplt.plot(x.numpy(), y_swish.numpy(), label='Swish (SwiGLU component)', linewidth=2)\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.title('Activation Functions Comparison', fontweight='bold')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 2: Feed-Forward Networks (15 mins)\n\nImplement FFN with different activation functions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class FFN_ReLU(nn.Module):\n    \"\"\"Standard FFN with ReLU\"\"\"\n    def __init__(self, d_model, d_ff):\n        super().__init__()\n        self.w1 = nn.Linear(d_model, d_ff)\n        self.w2 = nn.Linear(d_ff, d_model)\n        self.activation = F.relu\n    \n    def forward(self, x):\n        return self.w2(self.activation(self.w1(x)))\n\nclass FFN_GELU(nn.Module):\n    \"\"\"FFN with GELU (used in GPT, BERT)\"\"\"\n    def __init__(self, d_model, d_ff):\n        super().__init__()\n        self.w1 = nn.Linear(d_model, d_ff)\n        self.w2 = nn.Linear(d_ff, d_model)\n        self.activation = F.gelu\n    \n    def forward(self, x):\n        return self.w2(self.activation(self.w1(x)))\n\nclass FFN_SwiGLU(nn.Module):\n    \"\"\"FFN with SwiGLU (used in PaLM, LLaMA)\"\"\"\n    def __init__(self, d_model, d_ff):\n        super().__init__()\n        # SwiGLU uses 2/3 * d_ff for gate projection\n        self.w1 = nn.Linear(d_model, d_ff * 2)  # Gate and up projection\n        self.w2 = nn.Linear(d_ff, d_model)\n    \n    def forward(self, x):\n        x1, x2 = self.w1(x).chunk(2, dim=-1)\n        return self.w2(F.silu(x1) * x2)\n\n# Test all three\nd_model = 128\nd_ff = 512\nbatch_size = 2\nseq_len = 10\n\nx = torch.randn(batch_size, seq_len, d_model)\n\nffn_relu = FFN_ReLU(d_model, d_ff)\nffn_gelu = FFN_GELU(d_model, d_ff)\nffn_swiglu = FFN_SwiGLU(d_model, int(d_ff * 2/3))  # SwiGLU uses 2/3 size\n\nout_relu = ffn_relu(x)\nout_gelu = ffn_gelu(x)\nout_swiglu = ffn_swiglu(x)\n\nprint(f\"Input shape: {x.shape}\")\nprint(f\"ReLU FFN output: {out_relu.shape}\")\nprint(f\"GELU FFN output: {out_gelu.shape}\")\nprint(f\"SwiGLU FFN output: {out_swiglu.shape}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 3: Normalization Layers (15 mins)\n\nCompare LayerNorm and RMSNorm."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class RMSNorm(nn.Module):\n    \"\"\"Root Mean Square Layer Normalization (used in LLaMA)\"\"\"\n    def __init__(self, d_model, eps=1e-6):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(d_model))\n    \n    def forward(self, x):\n        # Compute RMS\n        rms = torch.sqrt(torch.mean(x**2, dim=-1, keepdim=True) + self.eps)\n        # Normalize and scale\n        return self.weight * (x / rms)\n\nclass LayerNorm(nn.Module):\n    \"\"\"Standard Layer Normalization\"\"\"\n    def __init__(self, d_model, eps=1e-6):\n        super().__init__()\n        self.norm = nn.LayerNorm(d_model, eps=eps)\n    \n    def forward(self, x):\n        return self.norm(x)\n\n# Compare\nd_model = 128\nbatch_size = 2\nseq_len = 10\n\nx = torch.randn(batch_size, seq_len, d_model)\n\nlayer_norm = LayerNorm(d_model)\nrms_norm = RMSNorm(d_model)\n\nout_ln = layer_norm(x)\nout_rms = rms_norm(x)\n\nprint(f\"Input shape: {x.shape}\")\nprint(f\"LayerNorm output shape: {out_ln.shape}\")\nprint(f\"RMSNorm output shape: {out_rms.shape}\")\nprint(f\"\\nLayerNorm mean: {out_ln.mean().item():.6f}, std: {out_ln.std().item():.6f}\")\nprint(f\"RMSNorm mean: {out_rms.mean().item():.6f}, std: {out_rms.std().item():.6f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Memory profiling comparison\nimport torch\n\ndef profile_memory(model, x, name):\n    if torch.cuda.is_available():\n        model = model.cuda()\n        x = x.cuda()\n        torch.cuda.reset_peak_memory_stats()\n        _ = model(x)\n        memory_mb = torch.cuda.max_memory_allocated() / 1024**2\n        return memory_mb\n    return 0\n\nd_model = 256\nd_ff = 1024\nbatch_size = 4\nseq_len = 32\n\nx = torch.randn(batch_size, seq_len, d_model)\n\nmodels = {\n    \"ReLU FFN\": FFN_ReLU(d_model, d_ff),\n    \"GELU FFN\": FFN_GELU(d_model, d_ff),\n    \"SwiGLU FFN\": FFN_SwiGLU(d_model, int(d_ff * 2/3)),\n}\n\nprint(\"Memory Usage (if CUDA available):\")\nfor name, model in models.items():\n    mem = profile_memory(model, x, name)\n    if mem > 0:\n        print(f\"  {name}: {mem:.2f} MB\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n\n\u2705 **SwiGLU**: More expressive than ReLU/GELU, used in modern models (LLaMA, PaLM)\n\n\u2705 **RMSNorm**: Simpler than LayerNorm (no mean centering), faster and more efficient\n\n\u2705 **FFN Architecture**: Typically 4x expansion (d_ff = 4 * d_model)\n\n\u2705 **Memory**: SwiGLU uses 2/3 * d_ff for gate, but often outperforms standard FFN\n\n## Next Steps\n\nContinue to **Module 1.4: Complete Transformer Block** to build a full transformer layer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}