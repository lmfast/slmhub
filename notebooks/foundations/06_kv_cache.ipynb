{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1.6: KV Cache & Attention Optimization\n\n**Goal**: Understand autoregressive generation optimization\n\n**Time**: 50 minutes\n\n**Concepts Covered**:\n- Naive generation (O(n\u00b2) problem)\n- KV cache implementation\n- Grouped Query Attention (GQA)\n- Multi-Head Latent Attention (MLA)\n- Benchmark: MHA vs GQA vs MLA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install torch numpy matplotlib seaborn transformers -q"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\nimport torch.nn as nn\nimport time\n\ntorch.manual_seed(42)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Generation (O(n\u00b2))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Simulate naive generation\ndef naive_generate(model, prompt, max_len=10):\n    sequence = prompt\n    for _ in range(max_len):\n        # Recompute attention for entire sequence each time\n        output = model(sequence)\n        next_token = output[:, -1:].argmax(dim=-1)\n        sequence = torch.cat([sequence, next_token], dim=1)\n    return sequence\n\nprint(\"Naive generation recomputes all previous tokens each step!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KV Cache Implementation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class CachedAttention(nn.Module):\n    def __init__(self, d_model, n_heads):\n        super().__init__()\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_k = d_model // n_heads\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n    \n    def forward(self, x, cache=None):\n        batch, seq, _ = x.shape\n        Q = self.W_q(x).view(batch, seq, self.n_heads, self.d_k).transpose(1, 2)\n        K = self.W_k(x).view(batch, seq, self.n_heads, self.d_k).transpose(1, 2)\n        V = self.W_v(x).view(batch, seq, self.n_heads, self.d_k).transpose(1, 2)\n        \n        if cache is not None:\n            # Concatenate with cache\n            K = torch.cat([cache['k'], K], dim=2)\n            V = torch.cat([cache['v'], V], dim=2)\n        \n        # Attention\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n        attn = F.softmax(scores, dim=-1)\n        out = torch.matmul(attn, V)\n        \n        # Update cache\n        new_cache = {'k': K, 'v': V}\n        \n        out = out.transpose(1, 2).contiguous().view(batch, seq, self.d_model)\n        return self.W_o(out), new_cache\n\nprint(\"KV cache stores K and V from previous tokens!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n\n\u2705 **Module Complete**\n\n## Next Steps\n\nContinue to the next module in the course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}