{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1.9: Training Optimizations\n\n**Goal**: Learn memory-efficient training techniques\n\n**Time**: 45 minutes\n\n**Concepts Covered**:\n- Gradient accumulation\n- Gradient checkpointing\n- Mixed precision training (AMP)\n- Memory vs compute trade-offs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install torch numpy matplotlib seaborn transformers -q"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\nimport torch.nn as nn\nfrom torch.cuda.amp import autocast, GradScaler\n\ntorch.manual_seed(42)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Accumulation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Simulate gradient accumulation\ndef train_with_accumulation(model, data, accumulation_steps=4):\n    optimizer = torch.optim.Adam(model.parameters())\n    optimizer.zero_grad()\n    \n    for i, batch in enumerate(data):\n        loss = model(batch)\n        loss = loss / accumulation_steps  # Scale loss\n        loss.backward()\n        \n        if (i + 1) % accumulation_steps == 0:\n            optimizer.step()\n            optimizer.zero_grad()\n    \n    print(f\"Effective batch size: {len(data) * accumulation_steps}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed Precision Training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "scaler = GradScaler()\n\ndef train_with_amp(model, data):\n    optimizer = torch.optim.Adam(model.parameters())\n    \n    for batch in data:\n        optimizer.zero_grad()\n        \n        with autocast():\n            loss = model(batch)\n        \n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n    \n    print(\"Mixed precision training reduces memory by ~50%!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n\n\u2705 **Module Complete**\n\n## Next Steps\n\nContinue to the next module in the course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}