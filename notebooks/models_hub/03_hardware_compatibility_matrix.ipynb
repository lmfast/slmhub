{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 6.3: Hardware Compatibility Matrix\n\n**Goal**: Calculate hardware requirements and compatibility\n\n**Time**: 45 minutes\n\n**Concepts Covered**:\n- Memory calculation functions\n- Speed estimation algorithms\n- GPU compatibility checker\n- Batch size calculator\n- Interactive requirements tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install torch transformers accelerate matplotlib seaborn numpy -q"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def calculate_memory_requirements(params, precision=\"fp16\", batch_size=1, seq_len=2048):\n    \"\"\"Calculate memory requirements for inference\"\"\"\n    bytes_per_param = {\n        \"fp32\": 4,\n        \"fp16\": 2,\n        \"bf16\": 2,\n        \"int8\": 1,\n        \"int4\": 0.5,\n    }[precision]\n    \n    # Model weights\n    model_memory_gb = params * bytes_per_param / 1e9\n    \n    # KV cache (approximate: 2 * params * batch * seq_len * bytes_per_param)\n    kv_cache_gb = params * 2 * batch_size * seq_len * bytes_per_param / 1e9\n    \n    # Activation memory (rough estimate)\n    activation_memory_gb = batch_size * seq_len * params * 0.1 * bytes_per_param / 1e9\n    \n    total_memory_gb = model_memory_gb + kv_cache_gb + activation_memory_gb\n    \n    return {\n        \"model_memory_gb\": model_memory_gb,\n        \"kv_cache_gb\": kv_cache_gb,\n        \"activation_memory_gb\": activation_memory_gb,\n        \"total_memory_gb\": total_memory_gb,\n    }\n\ndef check_gpu_compatibility(model_params, precision, gpu_memory_gb=16):\n    \"\"\"Check if model fits on GPU\"\"\"\n    reqs = calculate_memory_requirements(model_params, precision)\n    fits = reqs[\"total_memory_gb\"] <= gpu_memory_gb * 0.9  # 90% safety margin\n    return fits, reqs\n\n# Example\nmodel_params = 1_700_000_000  # 1.7B\ngpu_memory = 16  # T4 GPU\n\nfor precision in [\"fp16\", \"int8\", \"int4\"]:\n    fits, reqs = check_gpu_compatibility(model_params, precision, gpu_memory)\n    status = \"\u2705 Fits\" if fits else \"\u274c Too large\"\n    print(f\"{precision.upper()}: {status}\")\n    print(f\"  Total memory: {reqs['total_memory_gb']:.2f} GB\")\n    print()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n\n\u2705 **Module Complete**\n\n## Next Steps\n\nContinue to the next module in the course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}