{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# SLM Architecture Explorer\n",
                "\n",
                "This notebook helps explore the internal structure of Small Language Models using Hugging Face Transformers. Guide: [SLM Architecture](https://slmhub.gitbook.io/slmhub/docs/learn/fundamentals/architecture).\n",
                "\n",
                "## 1. Setup\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install transformers accelerate"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Inspect a Model Configuration\n",
                "We can see the layers, heads, and dimensions without downloading the full weights."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import AutoConfig\n",
                "\n",
                "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
                "\n",
                "config = AutoConfig.from_pretrained(model_id)\n",
                "print(config)\n",
                "\n",
                "print(\"\\n--- Key Architecture Details ---\")\n",
                "print(f\"Hidden Size (d_model): {config.hidden_size}\")\n",
                "print(f\"Number of Layers: {config.num_hidden_layers}\")\n",
                "print(f\"Attention Heads: {config.num_attention_heads}\")\n",
                "print(f\"Vocab Size: {config.vocab_size}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Compare with a Larger Model\n",
                "Let's compare Phi-3 (3.8B) with Llama-2-7b."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "llama_config = AutoConfig.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
                "\n",
                "print(f\"Phi-3 Layers: {config.num_hidden_layers} vs Llama-2 Layers: {llama_config.num_hidden_layers}\")\n",
                "print(f\"Phi-3 Hidden: {config.hidden_size} vs Llama-2 Hidden: {llama_config.hidden_size}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}