{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸš€ Your First SLM in 10 Minutes\n",
                "\n",
                "**Goal**: Get hands-on with a real Small Language Model in under 10 minutes!\n",
                "\n",
                "**What you'll do**:\n",
                "- âœ… Load SmolLM-135M (smallest model, runs anywhere)\n",
                "- âœ… Generate text\n",
                "- âœ… Experiment with sampling parameters\n",
                "- âœ… Understand what's happening under the hood\n",
                "\n",
                "**Hardware**: Runs on CPU or GPU (T4 recommended for speed)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Install Dependencies (1 min)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install transformers torch accelerate -q\n",
                "print(\"âœ… Dependencies installed!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Check Your Hardware (30 sec)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    device = \"cuda\"\n",
                "    gpu_name = torch.cuda.get_device_name(0)\n",
                "    vram = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
                "    print(f\"âœ… GPU Detected: {gpu_name}\")\n",
                "    print(f\"   VRAM: {vram:.2f} GB\")\n",
                "elif torch.backends.mps.is_available():\n",
                "    device = \"mps\"\n",
                "    print(\"âœ… Apple Silicon GPU (MPS) Detected\")\n",
                "else:\n",
                "    device = \"cpu\"\n",
                "    print(\"âš ï¸ No GPU detected. Using CPU (slower but works!)\")\n",
                "\n",
                "print(f\"\\nUsing device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Load SmolLM-135M (2 min)\n",
                "\n",
                "**SmolLM-135M** is the smallest model in the SmolLM family:\n",
                "- 135 million parameters\n",
                "- Trained on 2 trillion tokens\n",
                "- Only ~270MB in memory (FP16)\n",
                "- Runs on any hardware"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "\n",
                "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
                "\n",
                "print(f\"Loading {model_name}...\")\n",
                "\n",
                "# Load tokenizer\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "\n",
                "# Load model\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_name,\n",
                "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
                "    device_map=\"auto\" if device == \"cuda\" else None\n",
                ")\n",
                "\n",
                "if device == \"cpu\":\n",
                "    model = model.to(device)\n",
                "\n",
                "print(\"âœ… Model loaded successfully!\")\n",
                "print(f\"\\nModel info:\")\n",
                "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")\n",
                "print(f\"  Memory: ~{sum(p.numel() * p.element_size() for p in model.parameters()) / 1024**2:.0f} MB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Generate Your First Text! (1 min)\n",
                "\n",
                "Let's generate text with a simple prompt."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "prompt = \"The future of AI is\"\n",
                "\n",
                "# Tokenize input\n",
                "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
                "\n",
                "# Generate\n",
                "print(f\"Prompt: '{prompt}'\\n\")\n",
                "print(\"Generating...\")\n",
                "\n",
                "outputs = model.generate(\n",
                "    **inputs,\n",
                "    max_new_tokens=50,\n",
                "    do_sample=True,\n",
                "    temperature=0.7,\n",
                "    top_p=0.9\n",
                ")\n",
                "\n",
                "# Decode and print\n",
                "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "print(f\"\\nâœ¨ Generated:\\n{generated_text}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Experiment with Sampling Parameters (5 min)\n",
                "\n",
                "Let's see how different parameters affect generation:\n",
                "\n",
                "### Temperature\n",
                "- **Low (0.1-0.5)**: More focused, deterministic\n",
                "- **Medium (0.7-0.9)**: Balanced creativity\n",
                "- **High (1.0-2.0)**: More random, creative"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_with_params(prompt, temperature=0.7, top_k=50, top_p=0.9, max_tokens=50):\n",
                "    \"\"\"Generate text with custom parameters\"\"\"\n",
                "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
                "    \n",
                "    outputs = model.generate(\n",
                "        **inputs,\n",
                "        max_new_tokens=max_tokens,\n",
                "        do_sample=True,\n",
                "        temperature=temperature,\n",
                "        top_k=top_k,\n",
                "        top_p=top_p\n",
                "    )\n",
                "    \n",
                "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "\n",
                "# Test different temperatures\n",
                "prompt = \"Once upon a time\"\n",
                "\n",
                "print(\"ðŸŒ¡ï¸ Temperature Comparison:\\n\")\n",
                "\n",
                "for temp in [0.3, 0.7, 1.2]:\n",
                "    print(f\"Temperature = {temp}:\")\n",
                "    result = generate_with_params(prompt, temperature=temp, max_tokens=30)\n",
                "    print(f\"  {result}\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Top-k and Top-p (Nucleus Sampling)\n",
                "\n",
                "- **top_k**: Only consider top k most likely tokens\n",
                "- **top_p**: Consider smallest set of tokens with cumulative probability > p"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "prompt = \"The best way to learn programming is\"\n",
                "\n",
                "print(\"ðŸŽ¯ Sampling Strategy Comparison:\\n\")\n",
                "\n",
                "# Greedy (deterministic)\n",
                "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
                "outputs = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n",
                "print(\"Greedy (deterministic):\")\n",
                "print(f\"  {tokenizer.decode(outputs[0], skip_special_tokens=True)}\\n\")\n",
                "\n",
                "# Top-k = 10\n",
                "result = generate_with_params(prompt, temperature=0.7, top_k=10, top_p=1.0, max_tokens=30)\n",
                "print(\"Top-k = 10:\")\n",
                "print(f\"  {result}\\n\")\n",
                "\n",
                "# Top-p = 0.9 (nucleus)\n",
                "result = generate_with_params(prompt, temperature=0.7, top_k=0, top_p=0.9, max_tokens=30)\n",
                "print(\"Top-p = 0.9 (nucleus):\")\n",
                "print(f\"  {result}\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸŽ® Interactive Playground\n",
                "\n",
                "Try your own prompts and parameters!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Customize these!\n",
                "YOUR_PROMPT = \"Write a haiku about machine learning\"\n",
                "YOUR_TEMPERATURE = 0.8\n",
                "YOUR_TOP_P = 0.9\n",
                "YOUR_MAX_TOKENS = 50\n",
                "\n",
                "result = generate_with_params(\n",
                "    YOUR_PROMPT,\n",
                "    temperature=YOUR_TEMPERATURE,\n",
                "    top_p=YOUR_TOP_P,\n",
                "    max_tokens=YOUR_MAX_TOKENS\n",
                ")\n",
                "\n",
                "print(f\"Prompt: {YOUR_PROMPT}\")\n",
                "print(f\"\\nGenerated:\\n{result}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ” What's Happening Under the Hood?\n",
                "\n",
                "Let's peek at the tokenization and generation process."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text = \"Hello, world!\"\n",
                "\n",
                "# Tokenize\n",
                "tokens = tokenizer.encode(text)\n",
                "print(f\"Text: '{text}'\")\n",
                "print(f\"\\nToken IDs: {tokens}\")\n",
                "print(f\"Number of tokens: {len(tokens)}\")\n",
                "\n",
                "# Decode back\n",
                "print(f\"\\nToken breakdown:\")\n",
                "for token_id in tokens:\n",
                "    token_text = tokenizer.decode([token_id])\n",
                "    print(f\"  {token_id:5d} -> '{token_text}'\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# See model's next-token predictions\n",
                "prompt = \"The capital of France is\"\n",
                "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
                "\n",
                "with torch.no_grad():\n",
                "    outputs = model(**inputs)\n",
                "    logits = outputs.logits[0, -1, :]  # Last token's predictions\n",
                "\n",
                "# Get top 10 predictions\n",
                "top_k = 10\n",
                "probs = torch.softmax(logits, dim=-1)\n",
                "top_probs, top_indices = torch.topk(probs, top_k)\n",
                "\n",
                "print(f\"Prompt: '{prompt}'\")\n",
                "print(f\"\\nTop {top_k} next token predictions:\\n\")\n",
                "for i, (prob, idx) in enumerate(zip(top_probs, top_indices)):\n",
                "    token = tokenizer.decode([idx])\n",
                "    print(f\"  {i+1}. '{token}' (probability: {prob.item():.4f})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸŽ¯ Key Takeaways\n",
                "\n",
                "âœ… **You just ran a 135M parameter language model!**\n",
                "\n",
                "âœ… **Sampling parameters matter**:\n",
                "- Temperature controls randomness\n",
                "- top_k/top_p control diversity\n",
                "- Greedy = deterministic, sampling = creative\n",
                "\n",
                "âœ… **Tokenization is key**:\n",
                "- Text â†’ Token IDs â†’ Model â†’ Token IDs â†’ Text\n",
                "- Each token has a probability distribution\n",
                "\n",
                "âœ… **SmolLM-135M is tiny but capable**:\n",
                "- Only 270MB in memory\n",
                "- Trained on 2T tokens\n",
                "- Runs on any hardware\n",
                "\n",
                "## ðŸš€ Next Steps\n",
                "\n",
                "1. **Try larger models**: SmolLM-360M, SmolLM-1.7B\n",
                "2. **Learn fine-tuning**: Module 3.2 (customize for your task)\n",
                "3. **Build RAG**: Module 3.5 (add external knowledge)\n",
                "4. **Deploy**: Module 3.7 (production serving)\n",
                "\n",
                "## ðŸ’¡ Challenges\n",
                "\n",
                "1. Generate a story that starts with \"In a world where AI...\"\n",
                "2. Find the best temperature for creative writing vs factual Q&A\n",
                "3. Compare SmolLM-135M with GPT-2 (same size, different training)\n",
                "4. Build a simple chatbot loop (hint: append user input to conversation history)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}