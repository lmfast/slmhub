{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Fine-Tuning SLMs with QLoRA\n",
                "\n",
                "This notebook demonstrates how to fine-tune a model (Microsoft Phi-3 Mini) using QLoRA (Quantized Low-Rank Adaptation) on a free Google Colab GPU.\n",
                "Corresponding guide: [Fine-Tuning](https://slmhub.gitbook.io/slmhub/docs/learn/fundamentals/fine-tuning).\n",
                "\n",
                "## 1. Setup\n",
                "Install necessary libraries."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q transformers peft bitsandbytes trl datasets accelerate"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Model in 4-bit\n",
                "We use `bitsandbytes` to load the model in 4-bit precision to save memory. We choose `Phi-3-mini-4k-instruct` (3.8B) which comfortably fits on a free T4 GPU."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
                "from peft import prepare_model_for_kbit_training\n",
                "\n",
                "# Model ID\n",
                "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
                "\n",
                "# QLoRA Config\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
                "    bnb_4bit_use_double_quant=True\n",
                ")\n",
                "\n",
                "# Load Model\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_id,\n",
                "    quantization_config=bnb_config,\n",
                "    device_map=\"auto\",\n",
                "    trust_remote_code=True\n",
                ")\n",
                "\n",
                "# Enable gradient checkpointing to save memory\n",
                "model.gradient_checkpointing_enable()\n",
                "model = prepare_model_for_kbit_training(model)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Configure LoRA\n",
                "We attach low-rank adapters to the model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from peft import LoraConfig, get_peft_model\n",
                "\n",
                "lora_config = LoraConfig(\n",
                "    r=16,\n",
                "    lora_alpha=32,\n",
                "    target_modules=[\"k_proj\", \"q_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
                "    lora_dropout=0.05,\n",
                "    bias=\"none\",\n",
                "    task_type=\"CAUSAL_LM\"\n",
                ")\n",
                "\n",
                "model = get_peft_model(model, lora_config)\n",
                "model.print_trainable_parameters()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Dataset\n",
                "We use the standard ChatML format (`<|user|>` / `<|assistant|>`)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datasets import Dataset\n",
                "\n",
                "data = [\n",
                "    {\"messages\": [{\"role\": \"user\", \"content\": \"What are SLMs?\"}, {\"role\": \"assistant\", \"content\": \"SLMs are Small Language Models, typically under 10B parameters, designed for efficiency.\"}]},\n",
                "    {\"messages\": [{\"role\": \"user\", \"content\": \"Explain LoRA.\"}, {\"role\": \"assistant\", \"content\": \"LoRA (Low-Rank Adaptation) is a fine-tuning technique that updates only a small subset of parameters.\"}]},\n",
                "    {\"messages\": [{\"role\": \"user\", \"content\": \"Who created Phi-3?\"}, {\"role\": \"assistant\", \"content\": \"Microsoft created Phi-3.\"}]}\n",
                "] * 10  # Duplicate to simulate training\n",
                "\n",
                "dataset = Dataset.from_list(data)\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
                "tokenizer.pad_token = tokenizer.eos_token"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Train\n",
                "We use `SFTTrainer` which handles formatting automatically if using `messages`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from trl import SFTTrainer\n",
                "from transformers import TrainingArguments\n",
                "\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=\"./results\",\n",
                "    num_train_epochs=1,\n",
                "    per_device_train_batch_size=1,\n",
                "    gradient_accumulation_steps=4,\n",
                "    learning_rate=2e-4,\n",
                "    fp16=True,\n",
                "    logging_steps=1,\n",
                "    optim=\"paged_adamw_8bit\"\n",
                ")\n",
                "\n",
                "trainer = SFTTrainer(\n",
                "    model=model,\n",
                "    train_dataset=dataset,\n",
                "    tokenizer=tokenizer,\n",
                "    args=training_args,\n",
                ")\n",
                "\n",
                "trainer.train()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Inference & Save\n",
                "Test the fine-tuned model and save the adapter."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test\n",
                "inputs = tokenizer(\"<|user|>\\nWhat are SLMs?<|end|>\\n<|assistant|>\", return_tensors=\"pt\").to(\"cuda\")\n",
                "outputs = model.generate(**inputs, max_new_tokens=50)\n",
                "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
                "\n",
                "# Save Adapter\n",
                "trainer.save_model(\"./slmhub-adapter\")\n",
                "print(\"Adapter saved to ./slmhub-adapter\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}