{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Function Calling & Structured Output with SLMs\n",
                "\n",
                "This notebook demonstrates how to use function calling (tool use) and structured data extraction with Ollama, corresponding to the SLM Hub [Function Calling Guide](https://slmhub.gitbook.io/slmhub/docs/learn/concepts/function-calling).\n",
                "\n",
                "## 1. Setup Environment\n",
                "Install Ollama, OpenAI SDK (compatible with Ollama), and Instructor for structured outputs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install ollama openai instructor pydantic"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Install and Start Ollama\n",
                "We install the Ollama binary and run it in the background."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!curl -fsSL https://ollama.com/install.sh | sh"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import subprocess\n",
                "import time\n",
                "\n",
                "# Start Ollama server in background\n",
                "process = subprocess.Popen([\"ollama\", \"serve\"])\n",
                "time.sleep(5)\n",
                "print(\"Ollama started!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Pull Model\n",
                "Qwen2.5 is excellent for function calling. We'll use the 7B version (finetuned for tools)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!ollama pull qwen2.5:7b"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Basic Tool Calling Loop\n",
                "We define a dummy weather tool and let the model call it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import ollama\n",
                "\n",
                "# 1. Define Tool\n",
                "weather_tool = {\n",
                "  'type': 'function',\n",
                "  'function': {\n",
                "    'name': 'get_current_weather',\n",
                "    'description': 'Get the current weather for a city',\n",
                "    'parameters': {\n",
                "      'type': 'object',\n",
                "      'properties': {\n",
                "        'city': {\n",
                "          'type': 'string',\n",
                "          'description': 'The name of the city',\n",
                "        },\n",
                "      },\n",
                "      'required': ['city'],\n",
                "    },\n",
                "  },\n",
                "}\n",
                "\n",
                "# 2. Python Function\n",
                "def get_current_weather(city):\n",
                "    print(f\"Executing tool: get_current_weather({city})\")\n",
                "    return f\"The weather in {city} is 25 degrees Celsius and sunny.\"\n",
                "\n",
                "functions_map = {'get_current_weather': get_current_weather}\n",
                "\n",
                "# 3. Chat with Model\n",
                "query = \"How is the weather in Paris?\"\n",
                "print(f\"User: {query}\")\n",
                "\n",
                "messages = [{'role': 'user', 'content': query}]\n",
                "response = ollama.chat(\n",
                "    model='qwen2.5:7b',\n",
                "    messages=messages,\n",
                "    tools=[weather_tool]\n",
                ")\n",
                "\n",
                "# 4. Handle Tool Call\n",
                "if response['message'].get('tool_calls'):\n",
                "    for tool in response['message']['tool_calls']:\n",
                "        fn_name = tool['function']['name']\n",
                "        fn_args = tool['function']['arguments']\n",
                "        \n",
                "        # Call Python function\n",
                "        result = functions_map[fn_name](**fn_args)\n",
                "        \n",
                "        # Feed back to model\n",
                "        messages.append(response['message'])\n",
                "        messages.append({\n",
                "            'role': 'tool',\n",
                "            'content': result\n",
                "        })\n",
                "        \n",
                "        final_res = ollama.chat(model='qwen2.5:7b', messages=messages)\n",
                "        print(f\"Model: {final_res['message']['content']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Structured Output with Instructor\n",
                "Force the model to output valid JSON matching a Pydantic schema."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pydantic import BaseModel\n",
                "from openai import OpenAI\n",
                "import instructor\n",
                "\n",
                "class UserInfo(BaseModel):\n",
                "    name: str\n",
                "    age: int\n",
                "    skills: list[str]\n",
                "\n",
                "# Patch OpenAI client to use Instructor + Ollama\n",
                "client = instructor.patch(OpenAI(\n",
                "    base_url=\"http://localhost:11434/v1\",\n",
                "    api_key=\"ollama\"\n",
                "))\n",
                "\n",
                "# Extract data\n",
                "text = \"Gaurav is a 28 year old engineer who loves Typescript and Rust.\"\n",
                "user = client.chat.completions.create(\n",
                "    model=\"qwen2.5:7b\",\n",
                "    messages=[{\"role\": \"user\", \"content\": f\"Extract info: {text}\"}],\n",
                "    response_model=UserInfo\n",
                ")\n",
                "\n",
                "print(\"Extracted JSON:\")\n",
                "print(user.model_dump_json(indent=2))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}