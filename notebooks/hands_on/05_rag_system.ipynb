{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Build a RAG Pipeline with SLMs\n",
                "\n",
                "This notebook demonstrates how to build a Retrieval Augmented Generation system using ChromaDB, Sentence Transformers, and Ollama, corresponding to the SLM Hub [RAG Guide](https://slmhub.gitbook.io/slmhub/docs/learn/concepts/rag).\n",
                "\n",
                "## 1. Setup Environment\n",
                "We need to install Ollama (backend), the python client, and vector database tools."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install Python basic libraries\n",
                "!pip install chromadb sentence-transformers ollama"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Install and Start Ollama\n",
                "Since Colab is a fresh Linux environment, we install the Ollama binary and start it in the background."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!curl -fsSL https://ollama.com/install.sh | sh"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import subprocess\n",
                "import time\n",
                "\n",
                "# Start Ollama server in background\n",
                "process = subprocess.Popen([\"ollama\", \"serve\"])\n",
                "time.sleep(5)\n",
                "print(\"Ollama started!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Pull the Model\n",
                "We'll use `phi3` or `phi4` for generation. This might take a minute."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!ollama pull phi3"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Create Vector Store\n",
                "Index some dummy documents."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sentence_transformers import SentenceTransformer\n",
                "import chromadb\n",
                "\n",
                "# Initialize\n",
                "embedder = SentenceTransformer(\"BAAI/bge-small-en-v1.5\")\n",
                "client = chromadb.Client()\n",
                "try:\n",
                "    client.delete_collection(\"knowledge\")\n",
                "except:\n",
                "    pass\n",
                "collection = client.create_collection(\"knowledge\")\n",
                "\n",
                "# Documents\n",
                "documents = [\n",
                "    \"The refund policy allows full refunds within 30 days of purchase.\",\n",
                "    \"Standard shipping is free for all orders over $50 within the US.\",\n",
                "    \"Our support team is available 24/7 via live chat and email.\"\n",
                "]\n",
                "\n",
                "# Embed and Store\n",
                "embeddings = embedder.encode(documents)\n",
                "collection.add(\n",
                "    embeddings=embeddings.tolist(),\n",
                "    documents=documents,\n",
                "    ids=[f\"doc_{i}\" for i in range(len(documents))]\n",
                ")\n",
                "print(\"Documents indexed.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. RAG Function\n",
                "Combine retrieval and generation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import ollama\n",
                "\n",
                "def retrieve(query, k=1):\n",
                "    query_emb = embedder.encode(query)\n",
                "    results = collection.query(\n",
                "        query_embeddings=[query_emb.tolist()],\n",
                "        n_results=k\n",
                "    )\n",
                "    return results[\"documents\"][0]\n",
                "\n",
                "def ask_rag(question):\n",
                "    # 1. Retrieve\n",
                "    context_docs = retrieve(question)\n",
                "    context_text = \"\\n\".join(context_docs)\n",
                "    print(f\"[Retrieved Context]: {context_text}\\n\")\n",
                "    \n",
                "    # 2. Augment Prompt\n",
                "    prompt = f\"\"\"Answer the question based ONLY on the context below.\n",
                "    \n",
                "    Context:\n",
                "    {context_text}\n",
                "    \n",
                "    Question: {question}\n",
                "    Answer:\"\"\"\n",
                "    \n",
                "    # 3. Generate\n",
                "    response = ollama.chat(\n",
                "        model=\"phi3\",\n",
                "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
                "    )\n",
                "    return response[\"message\"][\"content\"]\n",
                "\n",
                "# Test\n",
                "q = \"What is the policy on getting money back?\"\n",
                "print(f\"[Question]: {q}\")\n",
                "print(f\"[Answer]: {ask_rag(q)}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}