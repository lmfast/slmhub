{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Tokenization Fundamentals\n",
                "\n",
                "This notebook covers the basics of tokenization using the Hugging Face `transformers` library, corresponding to the SLM Hub [Tokenization Guide](https://slmhub.gitbook.io/slmhub/docs/learn/fundamentals/tokenization).\n",
                "\n",
                "## 1. Setup\n",
                "We need to install the transformers library first."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install transformers"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Using AutoTokenizer\n",
                "Let's load a tokenizer from a real model (Microsoft's Phi-4) and see how it splits text."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import AutoTokenizer\n",
                "\n",
                "# Load the tokenizer\n",
                "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-4\")\n",
                "\n",
                "# Tokenize a simple sentence\n",
                "text = \"Small language models are powerful!\"\n",
                "tokens = tokenizer.tokenize(text)\n",
                "print(f\"Tokens: {tokens}\")\n",
                "\n",
                "# Convert to IDs (what the model sees)\n",
                "ids = tokenizer.encode(text)\n",
                "print(f\"Token IDs: {ids}\")\n",
                "\n",
                "# Decode back\n",
                "decoded = tokenizer.decode(ids)\n",
                "print(f\"Decoded: '{decoded}'\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Counting Tokens\n",
                "Functions to count tokens are essential for managing context windows."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def count_tokens(text, tokenizer):\n",
                "    return len(tokenizer.encode(text))\n",
                "\n",
                "sample_text = \"This is a sample text to count tokens.\"\n",
                "count = count_tokens(sample_text, tokenizer)\n",
                "print(f\"Text: '{sample_text}'\")\n",
                "print(f\"Token count: {count}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Batch Processing\n",
                "Processing multiple sentences at once with padding and truncation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "texts = [\"First sentence.\", \"Second sentence is slightly longer.\", \"Third.\"]\n",
                "\n",
                "batch = tokenizer(\n",
                "    texts,\n",
                "    padding=True,       # Pad shorter sequences\n",
                "    truncation=True,    # Truncate if too long\n",
                "    return_tensors=\"pt\" # Return PyTorch tensors\n",
                ")\n",
                "\n",
                "print(\"Batch Input IDs shape:\", batch['input_ids'].shape)\n",
                "print(\"Batch Input IDs:\\n\", batch['input_ids'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Special Tokens\n",
                "Inspect the special tokens used by the model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"BOS Token: {tokenizer.bos_token}\")\n",
                "print(f\"EOS Token: {tokenizer.eos_token}\")\n",
                "print(f\"PAD Token: {tokenizer.pad_token\")\n",
                "\n",
                "# Chat template example\n",
                "messages = [{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
                "try:\n",
                "    formatted = tokenizer.apply_chat_template(messages, tokenize=False)\n",
                "    print(f\"\\nFormatted Chat:\\n{formatted}\")\n",
                "except Exception as e:\n",
                "    print(\"This model might not have a chat template configured automatically in this version.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}