{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 5.3: Optimization Algorithms\n\n**Goal**: Implement optimizers from scratch\n\n**Time**: 90 minutes\n\n**Concepts Covered**:\n- Adam optimizer from scratch\n- AdamW implementation\n- Lion optimizer comparison\n- Learning rate schedules\n- Benchmark optimizers on training task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install torch transformers accelerate matplotlib seaborn numpy -q"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\nimport numpy as np\n\nclass AdamOptimizer:\n    \"\"\"Adam optimizer from scratch\"\"\"\n    def __init__(self, params, lr=1e-3, beta1=0.9, beta2=0.999, eps=1e-8):\n        self.params = list(params)\n        self.lr = lr\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.eps = eps\n        self.m = [torch.zeros_like(p) for p in self.params]  # First moment\n        self.v = [torch.zeros_like(p) for p in self.params]  # Second moment\n        self.t = 0\n    \n    def step(self):\n        self.t += 1\n        for i, param in enumerate(self.params):\n            if param.grad is None:\n                continue\n            \n            grad = param.grad\n            \n            # Update biased first moment estimate\n            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad\n            \n            # Update biased second moment estimate\n            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grad ** 2\n            \n            # Bias correction\n            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n            \n            # Update parameters\n            param.data -= self.lr * m_hat / (torch.sqrt(v_hat) + self.eps)\n    \n    def zero_grad(self):\n        for param in self.params:\n            if param.grad is not None:\n                param.grad.zero_()\n\nprint(\"Adam combines momentum (m) and adaptive learning rates (v)\")\nprint(\"AdamW adds weight decay decoupling\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n\n\u2705 **Module Complete**\n\n## Next Steps\n\nContinue to the next module in the course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}