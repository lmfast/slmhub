{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 5.6: Probability & Sampling\n\n**Goal**: Implement all sampling strategies\n\n**Time**: 75 minutes\n\n**Concepts Covered**:\n- Greedy decoding implementation\n- Temperature sampling with interactive demo\n- Top-k sampling\n- Top-p (nucleus) sampling\n- Compare all strategies with visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install torch transformers accelerate matplotlib seaborn numpy -q"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\nimport torch.nn.functional as F\nimport numpy as np\n\ndef greedy_decode(logits):\n    \"\"\"Greedy: always pick highest probability token\"\"\"\n    return torch.argmax(logits, dim=-1)\n\ndef temperature_sample(logits, temperature=1.0):\n    \"\"\"Temperature sampling\"\"\"\n    scaled_logits = logits / temperature\n    probs = F.softmax(scaled_logits, dim=-1)\n    return torch.multinomial(probs, 1)\n\ndef top_k_sample(logits, k=50):\n    \"\"\"Top-k sampling\"\"\"\n    top_k_logits, top_k_indices = torch.topk(logits, k, dim=-1)\n    top_k_probs = F.softmax(top_k_logits, dim=-1)\n    sampled_idx = torch.multinomial(top_k_probs, 1)\n    return top_k_indices.gather(-1, sampled_idx)\n\ndef top_p_sample(logits, p=0.9):\n    \"\"\"Top-p (nucleus) sampling\"\"\"\n    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n    sorted_probs = F.softmax(sorted_logits, dim=-1)\n    cumsum_probs = torch.cumsum(sorted_probs, dim=-1)\n    \n    # Remove tokens with cumulative probability > p\n    sorted_indices_to_remove = cumsum_probs > p\n    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n    sorted_indices_to_remove[..., 0] = 0\n    \n    # Set removed tokens to very negative value\n    sorted_logits[sorted_indices_to_remove] = float('-inf')\n    sorted_probs = F.softmax(sorted_logits, dim=-1)\n    \n    sampled_idx = torch.multinomial(sorted_probs, 1)\n    return sorted_indices.gather(-1, sampled_idx)\n\n# Example\nlogits = torch.randn(1, 1000)  # 1000 token vocabulary\n\nprint(\"Sampling strategies:\")\nprint(f\"Greedy: token {greedy_decode(logits).item()}\")\nprint(f\"Temperature (T=0.8): token {temperature_sample(logits, 0.8).item()}\")\nprint(f\"Top-k (k=50): token {top_k_sample(logits, 50).item()}\")\nprint(f\"Top-p (p=0.9): token {top_p_sample(logits, 0.9).item()}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n\n\u2705 **Module Complete**\n\n## Next Steps\n\nContinue to the next module in the course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}