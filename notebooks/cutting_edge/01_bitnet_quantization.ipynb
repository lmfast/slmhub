{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 11.1: BitNet - 1.58-bit Quantization\n\n**Goal**: Implement BitNet quantization with ternary weights (-1, 0, +1)\n\n**Time**: 120 minutes\n\n**Concepts Covered**:\n- Ternary weight training (-1, 0, +1)\n- BitLinear layer implementation\n- Straight-through estimator\n- Training from scratch with BitNet\n- Performance comparison (memory, speed, quality)\n- Energy efficiency analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install torch transformers accelerate matplotlib seaborn numpy -q"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# BitNet: 1.58-bit Quantization\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass BitLinear(nn.Module):\n    \"\"\"BitLinear layer with ternary weights (-1, 0, +1)\"\"\"\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        \n        # Full precision weights for training\n        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n        \n        # Scale factor\n        self.scale = nn.Parameter(torch.ones(1))\n    \n    def quantize_weights(self):\n        \"\"\"Quantize weights to ternary (-1, 0, +1)\"\"\"\n        # Calculate threshold (median of absolute values)\n        abs_weights = self.weight.abs()\n        threshold = abs_weights.median()\n        \n        # Quantize: sign(weight) if |weight| > threshold, else 0\n        quantized = torch.sign(self.weight) * (abs_weights > threshold).float()\n        \n        # Scale to match original weight magnitude\n        scale = self.weight.abs().mean() / quantized.abs().clamp(min=1e-8).mean()\n        \n        return quantized * scale\n    \n    def forward(self, x):\n        \"\"\"Forward pass with straight-through estimator\"\"\"\n        # During training: use full precision with STE\n        # During inference: use quantized weights\n        \n        if self.training:\n            # Straight-through estimator: quantize in forward, but use full precision gradient\n            quantized = self.quantize_weights()\n            return F.linear(x, quantized)\n        else:\n            # Inference: use quantized weights\n            quantized = self.quantize_weights()\n            return F.linear(x, quantized)\n\n# Test BitLinear\nbit_linear = BitLinear(128, 64)\nx = torch.randn(2, 10, 128)\n\noutput = bit_linear(x)\nprint(f\"Input shape: {x.shape}\")\nprint(f\"Output shape: {output.shape}\")\nprint(f\"\\nBitNet benefits:\")\nprint(\"- 1.58 bits per weight (ternary: -1, 0, +1)\")\nprint(\"- ~16x memory reduction vs FP16\")\nprint(\"- Faster inference (integer operations)\")\nprint(\"- Energy efficient\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Straight-Through Estimator (STE)\ndef ste_quantize(weights, bits=1):\n    \"\"\"Quantize with straight-through estimator\"\"\"\n    # Forward: quantize\n    quantized = torch.sign(weights) * (weights.abs() > weights.abs().median()).float()\n    \n    # Backward: pass through gradient (no quantization in backward)\n    return weights + (quantized - weights).detach()\n\n# Example STE usage\nx = torch.randn(4, 4, requires_grad=True)\nx_quantized = ste_quantize(x)\n\nprint(\"Straight-Through Estimator:\")\nprint(\"- Forward: quantized values\")\nprint(\"- Backward: full precision gradients\")\nprint(\"- Allows training with quantized weights\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n\n\u2705 **Module Complete**\n\n## Next Steps\n\nContinue to the next module in the course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}