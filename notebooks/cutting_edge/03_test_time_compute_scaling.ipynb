{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 11.3: Test-Time Compute Scaling\n\n**Goal**: Implement test-time compute scaling techniques for better quality\n\n**Time**: 90 minutes\n\n**Concepts Covered**:\n- Chain-of-thought prompting\n- Self-consistency (majority voting)\n- Best-of-N sampling\n- Interactive reasoning visualization\n- Quality vs compute trade-offs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install torch transformers accelerate matplotlib seaborn numpy -q"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Chain-of-Thought Prompting\ndef chain_of_thought_prompt(question):\n    \"\"\"Generate chain-of-thought prompt\"\"\"\n    return f\"\"\"{question}\n\nLet's think step by step:\n1. First, I need to understand what is being asked.\n2. Then, I'll break down the problem into smaller parts.\n3. I'll solve each part systematically.\n4. Finally, I'll combine the solutions.\n\nSolution: \"\"\"\n\n# Example\nquestion = \"If a train travels 60 miles in 1 hour, how far will it travel in 3 hours?\"\ncot_prompt = chain_of_thought_prompt(question)\n\nprint(\"Chain-of-Thought Prompt:\")\nprint(cot_prompt)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Self-Consistency (Majority Voting)\nimport collections\n\ndef self_consistency_generate(model, tokenizer, prompt, num_samples=5):\n    \"\"\"Generate multiple samples and use majority voting\"\"\"\n    samples = []\n    \n    for _ in range(num_samples):\n        inputs = tokenizer(prompt, return_tensors=\"pt\")\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=100,\n            do_sample=True,\n            temperature=0.7,\n            num_return_sequences=1,\n        )\n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        samples.append(response)\n    \n    # Extract answers (simplified - in practice, parse structured output)\n    answers = [sample.split(\"Answer:\")[-1].strip() for sample in samples]\n    \n    # Majority voting\n    answer_counts = collections.Counter(answers)\n    majority_answer = answer_counts.most_common(1)[0][0]\n    confidence = answer_counts[majority_answer] / len(answers)\n    \n    return {\n        \"answer\": majority_answer,\n        \"confidence\": confidence,\n        \"all_samples\": samples,\n        \"vote_distribution\": dict(answer_counts)\n    }\n\nprint(\"Self-Consistency:\")\nprint(\"- Generate multiple samples\")\nprint(\"- Use majority voting for final answer\")\nprint(\"- Higher confidence with more agreement\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Best-of-N Sampling\ndef best_of_n_sampling(model, tokenizer, prompt, n=10, scorer=None):\n    \"\"\"Generate N samples and return the best one\"\"\"\n    samples = []\n    scores = []\n    \n    for _ in range(n):\n        inputs = tokenizer(prompt, return_tensors=\"pt\")\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=100,\n            do_sample=True,\n            temperature=0.8,\n        )\n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        samples.append(response)\n        \n        # Score the response (e.g., using a reward model)\n        if scorer:\n            score = scorer(response)\n        else:\n            # Default: use length as proxy (longer = better for some tasks)\n            score = len(response)\n        \n        scores.append(score)\n    \n    # Return best sample\n    best_idx = scores.index(max(scores))\n    \n    return {\n        \"best_sample\": samples[best_idx],\n        \"best_score\": scores[best_idx],\n        \"all_samples\": samples,\n        \"all_scores\": scores,\n    }\n\nprint(\"Best-of-N Sampling:\")\nprint(\"- Generate N samples\")\nprint(\"- Score each sample\")\nprint(\"- Return the best one\")\nprint(\"- Quality improves with N, but compute increases\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n\n\u2705 **Module Complete**\n\n## Next Steps\n\nContinue to the next module in the course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}