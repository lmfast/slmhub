{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 11.5: Emergent Abilities & Scaling\n\n**Goal**: Understand and measure emergent abilities through scaling\n\n**Time**: 75 minutes\n\n**Concepts Covered**:\n- Measuring emergent abilities\n- Chinchilla scaling law implementation\n- Optimal compute allocation\n- Model size vs training data trade-offs\n- Emergence visualization across scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install torch transformers accelerate matplotlib seaborn numpy -q"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Chinchilla Scaling Law\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef chinchilla_optimal_allocation(compute_budget):\n    \"\"\"Calculate optimal model size and training tokens using Chinchilla law\"\"\"\n    # Chinchilla: N_opt = 0.6 * C^0.5, D_opt = 20 * C^0.5\n    # Where C is compute in FLOPs, N is parameters, D is tokens\n    \n    # Convert compute to FLOPs (approximate)\n    # 1 training step \u2248 6 * N * D FLOPs\n    \n    # Optimal allocation\n    N_opt = 0.6 * (compute_budget ** 0.5)  # Parameters\n    D_opt = 20 * (compute_budget ** 0.5)   # Tokens\n    \n    return {\n        \"parameters\": int(N_opt),\n        \"tokens\": int(D_opt),\n        \"compute_budget\": compute_budget,\n    }\n\n# Example\ncompute_budgets = [1e18, 1e19, 1e20, 1e21]  # FLOPs\n\nprint(\"Chinchilla Optimal Allocation:\")\nfor C in compute_budgets:\n    allocation = chinchilla_optimal_allocation(C)\n    print(f\"\\nCompute: {C:.1e} FLOPs\")\n    print(f\"  Optimal parameters: {allocation['parameters']:.2e}\")\n    print(f\"  Optimal tokens: {allocation['tokens']:.2e}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Measuring Emergent Abilities\ndef measure_emergent_ability(model_sizes, benchmark_scores, threshold=0.5):\n    \"\"\"Measure when abilities emerge (cross threshold)\"\"\"\n    emergence_points = {}\n    \n    for benchmark_name, scores in benchmark_scores.items():\n        # Find when score crosses threshold\n        for i, (size, score) in enumerate(zip(model_sizes, scores)):\n            if score >= threshold and (i == 0 or scores[i-1] < threshold):\n                emergence_points[benchmark_name] = {\n                    \"model_size\": size,\n                    \"score\": score,\n                    \"index\": i\n                }\n                break\n    \n    return emergence_points\n\n# Example data\nmodel_sizes = [135e6, 360e6, 1.7e9, 3.8e9, 7e9]  # Parameters\nbenchmark_scores = {\n    \"MMLU\": [0.25, 0.32, 0.42, 0.55, 0.68],\n    \"GSM8K\": [0.10, 0.18, 0.35, 0.52, 0.75],\n    \"HumanEval\": [0.05, 0.12, 0.28, 0.45, 0.62],\n}\n\nemergence = measure_emergent_ability(model_sizes, benchmark_scores, threshold=0.5)\n\nprint(\"Emergent Abilities (crossing 50% threshold):\")\nfor benchmark, info in emergence.items():\n    print(f\"  {benchmark}: {info['model_size']/1e9:.2f}B parameters (score: {info['score']:.2f})\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n\n\u2705 **Module Complete**\n\n## Next Steps\n\nContinue to the next module in the course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}