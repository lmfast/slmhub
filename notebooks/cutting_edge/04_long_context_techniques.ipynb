{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 11.4: Long Context Techniques\n\n**Goal**: Extend context length beyond training with YaRN and streaming\n\n**Time**: 90 minutes\n\n**Concepts Covered**:\n- YaRN (RoPE extension) implementation\n- Streaming inference with sliding window\n- Sink token preservation\n- Context length extension beyond training\n- Memory-efficient long-context handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install torch transformers accelerate matplotlib seaborn numpy -q"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# YaRN: Yet another RoPE extensioN\nimport torch\nimport torch.nn as nn\nimport math\n\ndef apply_yarn_scaling(rope_freqs, scale_factor, original_max_len, new_max_len):\n    \"\"\"Apply YaRN scaling to RoPE frequencies\"\"\"\n    # YaRN scales frequencies to extend context\n    # Scale factor typically: new_max_len / original_max_len\n    \n    # Low frequency components: scale down\n    # High frequency components: keep original\n    \n    scaled_freqs = rope_freqs.clone()\n    \n    # Find transition point\n    alpha = scale_factor\n    beta = 32  # Hyperparameter\n    \n    # Scale low frequencies\n    for i in range(len(rope_freqs)):\n        if rope_freqs[i] < beta:\n            scaled_freqs[i] = rope_freqs[i] / alpha\n        else:\n            # Keep high frequencies\n            scaled_freqs[i] = rope_freqs[i]\n    \n    return scaled_freqs\n\n# Example\noriginal_max_len = 2048\nnew_max_len = 8192\nscale_factor = new_max_len / original_max_len\n\n# Simulated RoPE frequencies\nrope_freqs = torch.linspace(0.1, 100, 64)\n\nscaled_freqs = apply_yarn_scaling(rope_freqs, scale_factor, original_max_len, new_max_len)\n\nprint(f\"YaRN Scaling:\")\nprint(f\"  Original max length: {original_max_len}\")\nprint(f\"  New max length: {new_max_len}\")\nprint(f\"  Scale factor: {scale_factor:.2f}\")\nprint(f\"  Low freq scaling: applied\")\nprint(f\"  High freq preservation: applied\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Streaming Inference with Sliding Window\nclass SlidingWindowInference:\n    def __init__(self, model, window_size=2048, stride=512):\n        self.model = model\n        self.window_size = window_size\n        self.stride = stride\n        self.kv_cache = None\n    \n    def process_long_sequence(self, tokens, max_new_tokens=100):\n        \"\"\"Process long sequence with sliding window\"\"\"\n        seq_len = len(tokens)\n        \n        if seq_len <= self.window_size:\n            # Fits in one window\n            return self.model.generate(tokens, max_new_tokens=max_new_tokens)\n        \n        # Sliding window approach\n        outputs = []\n        start_idx = 0\n        \n        while start_idx < seq_len:\n            end_idx = min(start_idx + self.window_size, seq_len)\n            window_tokens = tokens[start_idx:end_idx]\n            \n            # Process window\n            window_output = self.model.generate(\n                window_tokens,\n                max_new_tokens=max_new_tokens if end_idx == seq_len else 0\n            )\n            \n            outputs.append(window_output)\n            \n            # Move window\n            start_idx += self.stride\n        \n        # Combine outputs\n        return self._combine_outputs(outputs)\n    \n    def _combine_outputs(self, outputs):\n        \"\"\"Combine outputs from multiple windows\"\"\"\n        # Simple concatenation (in practice, use overlap handling)\n        return torch.cat(outputs, dim=0)\n\nprint(\"Sliding Window Inference:\")\nprint(\"- Process long sequences in chunks\")\nprint(\"- Maintain context across windows\")\nprint(\"- Memory efficient for long contexts\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n\n\u2705 **Module Complete**\n\n## Next Steps\n\nContinue to the next module in the course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}