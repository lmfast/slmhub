{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 10.3: Cost Optimization\n\n**Goal**: Optimize deployment costs through batching, quantization, and caching\n\n**Time**: 75 minutes\n\n**Concepts Covered**:\n- Cost breakdown analysis\n- Batch inference strategies\n- Dynamic batching implementation\n- Model quantization for cost reduction\n- Response caching\n- Auto-scaling strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install torch transformers accelerate matplotlib seaborn numpy -q"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cost Breakdown Analysis\ndef calculate_inference_cost(\n    model_size_gb,\n    requests_per_hour,\n    avg_tokens_per_request,\n    gpu_cost_per_hour=0.50,\n    tokens_per_second=50\n):\n    \"\"\"Calculate inference costs\"\"\"\n    # Compute time needed\n    total_tokens = requests_per_hour * avg_tokens_per_request\n    compute_hours = total_tokens / (tokens_per_second * 3600)\n    \n    # GPU costs\n    gpu_cost = compute_hours * gpu_cost_per_hour\n    \n    # Memory costs (if using cloud storage)\n    memory_cost_per_hour = model_size_gb * 0.01  # $0.01/GB/hour\n    \n    total_cost_per_hour = gpu_cost + memory_cost_per_hour\n    total_cost_per_month = total_cost_per_hour * 24 * 30\n    cost_per_request = total_cost_per_hour / requests_per_hour\n    \n    return {\n        \"gpu_cost_per_hour\": gpu_cost,\n        \"memory_cost_per_hour\": memory_cost_per_hour,\n        \"total_cost_per_hour\": total_cost_per_hour,\n        \"total_cost_per_month\": total_cost_per_month,\n        \"cost_per_request\": cost_per_request,\n    }\n\n# Example calculation\ncosts = calculate_inference_cost(\n    model_size_gb=4.5,  # SmolLM-1.7B in FP16\n    requests_per_hour=1000,\n    avg_tokens_per_request=200,\n)\n\nprint(\"Cost Breakdown:\")\nfor key, value in costs.items():\n    if \"cost\" in key:\n        print(f\"  {key}: ${value:.4f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Dynamic Batching\nimport asyncio\nfrom collections import deque\nfrom typing import List, Callable\n\nclass DynamicBatcher:\n    def __init__(self, batch_size: int, max_wait_ms: int = 50):\n        self.batch_size = batch_size\n        self.max_wait_ms = max_wait_ms\n        self.queue = deque()\n        self.processing = False\n    \n    async def add_request(self, request, callback: Callable):\n        \"\"\"Add request to batch queue\"\"\"\n        self.queue.append((request, callback))\n        \n        if len(self.queue) >= self.batch_size:\n            await self._process_batch()\n        elif not self.processing:\n            asyncio.create_task(self._wait_and_process())\n    \n    async def _wait_and_process(self):\n        \"\"\"Wait for max_wait_ms then process batch\"\"\"\n        self.processing = True\n        await asyncio.sleep(self.max_wait_ms / 1000)\n        \n        if self.queue:\n            await self._process_batch()\n        \n        self.processing = False\n    \n    async def _process_batch(self):\n        \"\"\"Process current batch\"\"\"\n        if not self.queue:\n            return\n        \n        batch = []\n        callbacks = []\n        \n        while self.queue and len(batch) < self.batch_size:\n            request, callback = self.queue.popleft()\n            batch.append(request)\n            callbacks.append(callback)\n        \n        # Process batch (simulated)\n        results = await self._inference_batch(batch)\n        \n        # Call callbacks\n        for callback, result in zip(callbacks, results):\n            callback(result)\n    \n    async def _inference_batch(self, batch):\n        \"\"\"Run inference on batch\"\"\"\n        # Simulated batch inference\n        await asyncio.sleep(0.1)\n        return [f\"result_{i}\" for i in range(len(batch))]\n\nprint(\"Dynamic batching:\")\nprint(\"- Collects requests over time window\")\nprint(\"- Processes when batch is full or timeout\")\nprint(\"- Improves throughput and reduces costs\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Response Caching\nfrom functools import lru_cache\nimport hashlib\nimport json\n\nclass ResponseCache:\n    def __init__(self, max_size=1000):\n        self.cache = {}\n        self.max_size = max_size\n        self.hits = 0\n        self.misses = 0\n    \n    def _cache_key(self, prompt, **kwargs):\n        \"\"\"Generate cache key from prompt and parameters\"\"\"\n        key_data = {\"prompt\": prompt, **kwargs}\n        key_str = json.dumps(key_data, sort_keys=True)\n        return hashlib.md5(key_str.encode()).hexdigest()\n    \n    def get(self, prompt, **kwargs):\n        \"\"\"Get cached response\"\"\"\n        key = self._cache_key(prompt, **kwargs)\n        \n        if key in self.cache:\n            self.hits += 1\n            return self.cache[key]\n        \n        self.misses += 1\n        return None\n    \n    def set(self, prompt, response, **kwargs):\n        \"\"\"Cache response\"\"\"\n        key = self._cache_key(prompt, **kwargs)\n        \n        if len(self.cache) >= self.max_size:\n            # Remove oldest entry (simple FIFO)\n            oldest_key = next(iter(self.cache))\n            del self.cache[oldest_key]\n        \n        self.cache[key] = response\n    \n    def hit_rate(self):\n        \"\"\"Calculate cache hit rate\"\"\"\n        total = self.hits + self.misses\n        if total == 0:\n            return 0.0\n        return self.hits / total\n\n# Example usage\ncache = ResponseCache()\n\n# Cache a response\ncache.set(\"What is AI?\", \"AI is artificial intelligence...\", temperature=0.7)\n\n# Retrieve from cache\ncached = cache.get(\"What is AI?\", temperature=0.7)\nprint(f\"Cache hit: {cached is not None}\")\nprint(f\"Hit rate: {cache.hit_rate():.2%}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n\n\u2705 **Module Complete**\n\n## Next Steps\n\nContinue to the next module in the course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}