{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 10.1: Serving Infrastructure\n\n**Goal**: Set up production-ready serving infrastructure with vLLM and FastAPI\n\n**Time**: 90 minutes\n\n**Concepts Covered**:\n- vLLM production setup with PagedAttention\n- Async API server with FastAPI\n- Batch inference optimization\n- PagedAttention KV cache management\n- Throughput benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install torch transformers accelerate matplotlib seaborn numpy -q"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# vLLM Production Setup\nimport subprocess\nimport json\n\n# Install vLLM\n# !pip install vllm fastapi uvicorn -q\n\nprint(\"vLLM provides:\")\nprint(\"- PagedAttention for efficient KV cache\")\nprint(\"- Continuous batching for high throughput\")\nprint(\"- Async API support\")\nprint(\"- Production-ready serving infrastructure\")\n\n# Example vLLM server setup\nvllm_config = {\n    \"model\": \"HuggingFaceTB/SmolLM2-1.7B\",\n    \"tensor_parallel_size\": 1,\n    \"gpu_memory_utilization\": 0.9,\n    \"max_model_len\": 2048,\n    \"dtype\": \"float16\",\n}\n\nprint(f\"\\nvLLM Config: {json.dumps(vllm_config, indent=2)}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# FastAPI Async Server Example\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nimport asyncio\n\napp = FastAPI(title=\"SLM Inference API\")\n\nclass GenerationRequest(BaseModel):\n    prompt: str\n    max_tokens: int = 100\n    temperature: float = 0.7\n    top_p: float = 0.9\n\nclass GenerationResponse(BaseModel):\n    text: str\n    tokens_generated: int\n    latency_ms: float\n\n@app.post(\"/generate\", response_model=GenerationResponse)\nasync def generate(request: GenerationRequest):\n    \"\"\"Generate text from prompt\"\"\"\n    # In production, this would call vLLM engine\n    # For demo purposes, we show the structure\n    import time\n    start = time.time()\n    \n    # Simulated generation\n    await asyncio.sleep(0.1)  # Simulate inference time\n    \n    latency = (time.time() - start) * 1000\n    \n    return GenerationResponse(\n        text=\"Generated text...\",\n        tokens_generated=request.max_tokens,\n        latency_ms=latency\n    )\n\n@app.get(\"/health\")\nasync def health():\n    return {\"status\": \"healthy\"}\n\nprint(\"FastAPI server structure created!\")\nprint(\"Run with: uvicorn main:app --host 0.0.0.0 --port 8000\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Batch Inference Optimization\nimport torch\nimport time\n\ndef batch_inference(model, tokenizer, prompts, batch_size=8):\n    \"\"\"Optimized batch inference\"\"\"\n    results = []\n    \n    for i in range(0, len(prompts), batch_size):\n        batch = prompts[i:i+batch_size]\n        \n        # Tokenize batch\n        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n        \n        # Generate\n        with torch.no_grad():\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=100,\n                do_sample=True,\n                temperature=0.7,\n            )\n        \n        # Decode\n        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n        results.extend(decoded)\n    \n    return results\n\nprint(\"Batch inference improves throughput by:\")\nprint(\"- Processing multiple requests together\")\nprint(\"- Better GPU utilization\")\nprint(\"- Reduced overhead per request\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n\n\u2705 **Module Complete**\n\n## Next Steps\n\nContinue to the next module in the course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}