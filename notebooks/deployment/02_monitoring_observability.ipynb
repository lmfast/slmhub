{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 10.2: Monitoring & Observability\n\n**Goal**: Set up comprehensive monitoring and observability for production deployments\n\n**Time**: 90 minutes\n\n**Concepts Covered**:\n- Prometheus metrics integration\n- Grafana dashboard setup\n- Structured logging with structlog\n- Latency tracking (p50, p90, p99)\n- Error rate monitoring\n- GPU utilization tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install torch transformers accelerate matplotlib seaborn numpy -q"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Prometheus Metrics Integration\nfrom prometheus_client import Counter, Histogram, Gauge, start_http_server\nimport time\n\n# Define metrics\nrequest_count = Counter('inference_requests_total', 'Total inference requests')\nrequest_latency = Histogram('inference_latency_seconds', 'Inference latency')\ngpu_utilization = Gauge('gpu_utilization_percent', 'GPU utilization percentage')\nerror_count = Counter('inference_errors_total', 'Total inference errors')\n\ndef record_inference(latency_seconds, success=True):\n    \"\"\"Record inference metrics\"\"\"\n    request_count.inc()\n    request_latency.observe(latency_seconds)\n    \n    if not success:\n        error_count.inc()\n    \n    # Simulate GPU utilization\n    gpu_utilization.set(85.5)\n\n# Start Prometheus metrics server\n# start_http_server(8000)\n\nprint(\"Prometheus metrics defined:\")\nprint(\"- request_count: Total requests\")\nprint(\"- request_latency: Latency histogram\")\nprint(\"- gpu_utilization: GPU usage\")\nprint(\"- error_count: Error tracking\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Structured Logging with structlog\nimport structlog\nimport logging\n\n# Configure structlog\nstructlog.configure(\n    processors=[\n        structlog.stdlib.filter_by_level,\n        structlog.stdlib.add_logger_name,\n        structlog.stdlib.add_log_level,\n        structlog.stdlib.PositionalArgumentsFormatter(),\n        structlog.processors.TimeStamper(fmt=\"iso\"),\n        structlog.processors.StackInfoRenderer(),\n        structlog.processors.format_exc_info,\n        structlog.processors.UnicodeDecoder(),\n        structlog.processors.JSONRenderer()\n    ],\n    context_class=dict,\n    logger_factory=structlog.stdlib.LoggerFactory(),\n    wrapper_class=structlog.stdlib.BoundLogger,\n    cache_logger_on_first_use=True,\n)\n\nlogger = structlog.get_logger()\n\n# Example logging\nlogger.info(\"inference_request\", \n            model=\"SmolLM-1.7B\",\n            prompt_length=50,\n            max_tokens=100,\n            user_id=\"user123\")\n\nlogger.error(\"inference_failed\",\n             error=\"CUDA out of memory\",\n             model=\"SmolLM-1.7B\",\n             batch_size=8)\n\nprint(\"Structured logging provides:\")\nprint(\"- JSON-formatted logs\")\nprint(\"- Contextual information\")\nprint(\"- Easy parsing and analysis\")\nprint(\"- Better debugging\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Latency Percentiles\nimport numpy as np\nfrom collections import deque\n\nclass LatencyTracker:\n    def __init__(self, window_size=1000):\n        self.latencies = deque(maxlen=window_size)\n    \n    def record(self, latency_ms):\n        self.latencies.append(latency_ms)\n    \n    def get_percentiles(self):\n        if not self.latencies:\n            return {}\n        \n        latencies_array = np.array(self.latencies)\n        return {\n            \"p50\": np.percentile(latencies_array, 50),\n            \"p90\": np.percentile(latencies_array, 90),\n            \"p95\": np.percentile(latencies_array, 95),\n            \"p99\": np.percentile(latencies_array, 99),\n            \"mean\": np.mean(latencies_array),\n            \"max\": np.max(latencies_array),\n        }\n\n# Example usage\ntracker = LatencyTracker()\nfor _ in range(100):\n    # Simulate latency measurements\n    latency = np.random.lognormal(mean=3, sigma=0.5)  # ms\n    tracker.record(latency)\n\npercentiles = tracker.get_percentiles()\nprint(\"Latency Percentiles (ms):\")\nfor key, value in percentiles.items():\n    print(f\"  {key}: {value:.2f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n\n\u2705 **Module Complete**\n\n## Next Steps\n\nContinue to the next module in the course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}