---
title: llama.cpp
description: Efficient CPU and cross-platform inference for SLMs using GGUF format
---

import ColabButton from '../../../../../components/ColabButton.astro';

<ColabButton path="docs/deploy/quickstarts/llama_cpp.ipynb" />

# llama.cpp

llama.cpp enables running SLMs on virtually any hardware - CPUs, GPUs, Raspberry Pi, phones. It uses the GGUF format for optimized performance.

## Quick Start

### Installation

```bash
# Clone and build
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
make -j

# Or with GPU support (CUDA)
make -j LLAMA_CUDA=1

# Or with Metal (Apple Silicon)
make -j LLAMA_METAL=1
```

### Download a Model

```bash
# GGUF models from HuggingFace
wget https://huggingface.co/TheBloke/phi-3-mini-4k-instruct-GGUF/resolve/main/phi-3-mini-4k-instruct.Q4_K_M.gguf
```

### Run Inference

```bash
./llama-cli -m phi-3-mini-4k-instruct.Q4_K_M.gguf \
  -p "Explain small language models:" \
  -n 256
```

## Understanding GGUF

GGUF (GPT-Generated Unified Format) is the standard format for llama.cpp:

### Quantization Levels

| Quant | Bits | Size (7B) | Quality | Speed |
|-------|------|-----------|---------|-------|
| Q2_K | 2-3 | ~2.5 GB | ⭐⭐ | ⚡⚡⚡⚡⚡ |
| Q3_K_M | 3-4 | ~3.0 GB | ⭐⭐⭐ | ⚡⚡⚡⚡ |
| Q4_0 | 4 | ~3.5 GB | ⭐⭐⭐ | ⚡⚡⚡⚡ |
| Q4_K_M | 4-5 | ~4.0 GB | ⭐⭐⭐⭐ | ⚡⚡⚡ |
| Q5_K_M | 5 | ~4.5 GB | ⭐⭐⭐⭐⭐ | ⚡⚡⚡ |
| Q8_0 | 8 | ~7.0 GB | ⭐⭐⭐⭐⭐ | ⚡⚡ |
| F16 | 16 | ~14 GB | Perfect | ⚡ |

**Recommended:** `Q4_K_M` - best balance of quality and size

### Naming Convention

```
model-name.Q4_K_M.gguf
           │ │ │
           │ │ └── Size: S(mall), M(edium), L(arge)
           │ └──── Type: K(eep important weights higher precision)
           └────── Bits: 4-bit quantization
```

## CLI Usage

### Basic Generation

```bash
./llama-cli -m model.gguf \
  -p "Your prompt here" \
  -n 256 \                  # Max tokens to generate
  --temp 0.7 \              # Temperature
  --top-p 0.9               # Top-p sampling
```

### Interactive Chat

```bash
./llama-cli -m model.gguf \
  -i \                      # Interactive mode
  --color \                 # Colored output
  -r "User:" \              # Reverse prompt (user indicator)
  --in-prefix "User: " \
  --in-suffix "Assistant: "
```

### With System Prompt

```bash
./llama-cli -m model.gguf \
  --system-prompt "You are a helpful coding assistant." \
  -p "Write a Python function for fibonacci"
```

## Server Mode

Run as an API server (OpenAI-compatible):

```bash
./llama-server -m model.gguf \
  --host 0.0.0.0 \
  --port 8080 \
  -c 4096 \             # Context size
  -ngl 99               # GPU layers (0 for CPU only)
```

### API Usage

```bash
curl http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "model.gguf",
    "messages": [
      {"role": "user", "content": "Hello!"}
    ]
  }'
```

### Python Client

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8080/v1",
    api_key="not-needed"
)

response = client.chat.completions.create(
    model="model.gguf",
    messages=[{"role": "user", "content": "Explain AI"}]
)
print(response.choices[0].message.content)
```

## GPU Acceleration

### CUDA (NVIDIA)

```bash
# Build with CUDA
make clean
make -j LLAMA_CUDA=1

# Run with GPU layers
./llama-cli -m model.gguf \
  -ngl 99 \        # All layers on GPU
  -p "Hello"

# Partial GPU offload (low VRAM)
./llama-cli -m model.gguf \
  -ngl 20 \        # 20 layers on GPU, rest on CPU
  -p "Hello"
```

### Metal (Apple Silicon)

```bash
# Build with Metal
make clean
make -j LLAMA_METAL=1

# Run (auto-uses GPU)
./llama-cli -m model.gguf -p "Hello"
```

### Vulkan (Cross-platform)

```bash
make clean
make -j LLAMA_VULKAN=1
```

## Converting Models

### From HuggingFace to GGUF

```bash
# Install requirements
pip install -r requirements.txt

# Convert
python convert_hf_to_gguf.py /path/to/model \
  --outfile model.gguf \
  --outtype f16

# Quantize
./llama-quantize model.gguf model-Q4_K_M.gguf Q4_K_M
```

### From Safetensors

```bash
python convert_hf_to_gguf.py /path/to/safetensors/dir \
  --outfile model.gguf
```

## Performance Optimization

### CPU Optimization

```bash
# Set thread count
./llama-cli -m model.gguf \
  -t 8 \              # CPU threads
  -tb 4 \             # Batch threads
  -p "Hello"

# Enable SIMD (usually auto-detected)
# Rebuild with specific flags
make -j LLAMA_AVX2=1 LLAMA_F16C=1
```

### Memory Optimization

```bash
# Memory-map model (lower RAM usage)
./llama-cli -m model.gguf \
  --mlock \           # Keep in RAM
  -p "Hello"

# Reduce context for lower memory
./llama-cli -m model.gguf \
  -c 2048 \           # Smaller context
  -p "Hello"
```

### Batch Processing

```bash
# Process multiple prompts
./llama-cli -m model.gguf \
  -f prompts.txt \    # File with prompts
  -b 512              # Batch size
```

## Platform-Specific Guides

### Raspberry Pi 5

```bash
# Use highly quantized model
wget https://.../model.Q4_0.gguf

# Run with limited threads
./llama-cli -m model.Q4_0.gguf \
  -t 4 \
  -c 2048 \
  -p "Hello"
```

### Android (via Termux)

```bash
# In Termux
pkg install git cmake

git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
make -j4

# Use small model
./llama-cli -m SmolLM2-135M.Q4_0.gguf -p "Hi"
```

### iOS/macOS (Swift)

```swift
// Using llama.cpp Swift bindings
import LlamaKit

let model = try LlamaModel(path: "model.gguf")
let response = try model.generate(prompt: "Hello")
```

## Python Bindings

### llama-cpp-python

```bash
pip install llama-cpp-python

# With CUDA
CMAKE_ARGS="-DLLAMA_CUDA=on" pip install llama-cpp-python
```

```python
from llama_cpp import Llama

llm = Llama(
    model_path="model.gguf",
    n_ctx=4096,
    n_gpu_layers=99  # GPU layers
)

output = llm(
    "Explain machine learning:",
    max_tokens=256,
    temperature=0.7
)
print(output["choices"][0]["text"])
```

### Chat Format

```python
from llama_cpp import Llama

llm = Llama(
    model_path="model.gguf",
    chat_format="chatml"  # or "llama-2", "alpaca", etc.
)

response = llm.create_chat_completion(
    messages=[
        {"role": "system", "content": "You are helpful."},
        {"role": "user", "content": "What is AI?"}
    ]
)
print(response["choices"][0]["message"]["content"])
```

## llama.cpp vs Alternatives

| Feature | llama.cpp | Ollama | vLLM |
|---------|-----------|--------|------|
| CPU performance | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐ |
| GPU performance | ⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ |
| Cross-platform | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐ |
| Edge devices | ⭐⭐⭐⭐⭐ | ⭐⭐ | ❌ |
| Ease of use | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ |
| Python bindings | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ |

## Troubleshooting

### Slow Performance

```bash
# Check thread count
./llama-cli -m model.gguf -t $(nproc) -p "Hi"

# Try different quantization
# Q4_0 is faster than Q4_K_M

# Reduce context
-c 2048
```

### Out of Memory

```bash
# Use smaller quantization
# Q2_K or Q3_K_S

# Reduce GPU layers
-ngl 10  # Only 10 layers on GPU

# Reduce context
-c 1024
```

### Model Not Loading

```bash
# Check GGUF version (must be v2+)
file model.gguf

# Re-download or reconvert
```

## Next Steps

- Learn [Quantization](/slmhub/docs/learn/fundamentals/quantization/) theory
- Explore [Transformers.js](/slmhub/docs/deploy/quickstarts/transformers-js/) for browser
- Check [Edge Deployment](/slmhub/docs/deploy/patterns/) patterns
