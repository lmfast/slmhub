---
title: Ollama
description: "The simplest local runtime for SLMs."
---


import NotebookWidget from '../../../../../components/NotebookWidget.astro';

<NotebookWidget 
  notebookPath="notebooks/ollama_quickstart.ipynb"
  title="Try Ollama in Colab"
  description="Don't want to install locally? Run Ollama + Phi-4 directly in your browser with a free GPU."
/>

## Why use it

- Lowest friction local dev loop.
- Great for prototyping an OpenAI-compatible interface.

## Run

```bash
ollama run phi4
```

## Connect as an API

Many setups expose:

```bash
curl http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi4",
    "messages": [{"role": "user", "content": "Give me a 1-sentence summary of SLMs."}]
  }'
```

## Production note

Ollama is amazing for dev. For high concurrency and throughput, use vLLM: `deploy/quickstarts/vllm.md`.


