---
title: "Datasets - The Art & Science of Training Data"
description: "Understanding data quality, curation pipelines, and domain-specific datasets for SLM training"
---

import ColabButton from '../../../../../components/ColabButton.astro';

<ColabButton path="docs/learn/concepts/datasets.ipynb" />

# Datasets: The Art & Science of Training Data

High-quality data is the foundation of great models. This guide covers data curation, quality filtering, and domain-specific datasets.

## Why Data Quality > Quantity

### Case Study: DCLM (DataComp-LM)

**Discovery:** Model-based filtering beats web crawling

**Experiment:** Train 7B models on different datasets

| Dataset | Size | MMLU | Key Insight |
|---------|------|------|-------------|
| RefinedWeb | 600B tokens | 61.2 | Larger but noisy |
| DCLM-Baseline | 300B tokens | 64.7 | Filtered with heuristics |
| DCLM-Pool | 240B tokens | 68.1 | ML-based filtering wins! |

**The Secret:** Use small model to score data quality, train large model on top-quality subset

## Dataset Curation Pipeline

### Step 1: Raw Data Collection

**Sources:**
- Common Crawl (web pages)
- GitHub (code)
- arXiv (papers)
- Wikipedia (curated knowledge)
- Books (Project Gutenberg)

### Step 2: Deduplication

**Problem:** Repetitive data wastes compute & causes memorization

```python
from datasketch import MinHash

def deduplicate(documents):
    seen_hashes = set()
    unique_docs = []
    
    for doc in documents:
        minhash = MinHash()
        for word in doc.split():
            minhash.update(word.encode())
        
        hash_val = minhash.digest()
        if hash_val not in seen_hashes:
            unique_docs.append(doc)
            seen_hashes.add(hash_val)
    
    return unique_docs
```

### Step 3: Quality Filtering

**Method 1: Heuristic Filters**

```python
def quality_filter(text):
    # Remove if too short
    if len(text.split()) < 50:
        return False
    
    # Remove if mostly non-alphanumeric
    if sum(c.isalnum() for c in text) / len(text) < 0.8:
        return False
    
    # Remove if repetitive
    if has_repetitive_ngrams(text, n=10, threshold=0.3):
        return False
    
    return True
```

**Method 2: Classifier-Based (DCLM approach)**

```python
from fasttext import train_supervised

# Train classifier on curated vs random data
model = train_supervised(
    'quality_data.txt',  # High-quality examples
    'random_data.txt'    # Random web data
)

# Score all data
scores = [model.predict(doc)[1][0] for doc in corpus]

# Keep top 60%
threshold = percentile(scores, 40)
high_quality = [d for d, s in zip(corpus, scores) if s > threshold]
```

### Step 4: Decontamination

**Critical:** Remove evaluation sets from training data!

```python
from nltk.util import ngrams

def is_contaminated(train_doc, eval_sets, n=13):
    train_ngrams = set(ngrams(train_doc.split(), n))
    
    for eval_doc in eval_sets:
        eval_ngrams = set(ngrams(eval_doc.split(), n))
        overlap = len(train_ngrams & eval_ngrams)
        
        if overlap > 0:
            return True
    return False
```

## Domain-Specific Datasets

### Code Datasets

**The Stack v2 (2024):**
- 900+ programming languages
- 67 million files
- Deduplicated & filtered
- Respects opt-out requests

**Key Curation Steps:**
1. Remove autogenerated code
2. Filter by stars/activity
3. Detect & remove minified JS
4. Balance languages

### Math Datasets

**NuminaMath (2024):**
- 860K math problems with solutions
- Competition-grade difficulty
- Step-by-step reasoning
- Multiple solution methods

**Curation Focus:**
- Verify solutions programmatically
- Include reasoning chains
- Cover all difficulty levels

### Multilingual Datasets

**CulturaX:**
- 6.3 trillion tokens
- 167 languages
- Quality-filtered per language
- Preserves low-resource languages

**Challenges:**
- Tokenizer coverage
- Script diversity
- Cultural appropriateness

## Synthetic Data Generation

### The GPT-Teacher Paradigm

```python
# Generate training data with GPT-4
prompts = [
    "Explain {concept} to a beginner",
    "Write a {difficulty} coding problem about {topic}",
    "Create a multiple-choice question on {subject}"
]

# Generate 10K examples
for i in range(10000):
    concept = random.choice(concepts)
    difficulty = random.choice(['easy', 'medium', 'hard'])
    topic = random.choice(topics)
    
    prompt = random.choice(prompts).format(
        concept=concept, difficulty=difficulty, topic=topic
    )
    
    response = gpt4.generate(prompt)
    dataset.append({'input': prompt, 'output': response})

# Fine-tune SLM on synthetic data
fine_tune(slm, dataset)
```

**Success Stories:**
- Phi series: Trained on synthetic textbooks
- WizardCoder: Evol-Instruct synthetic code
- Orca: Complex reasoning from GPT-4

**Caution:** Synthetic data can inherit biases & errors from teacher model

## Practical Project: Curate Your Own Dataset

**Step-by-step:**
1. Collect data (scrape, download, generate)
2. Deduplicate (exact + fuzzy)
3. Filter for quality
4. Check contamination
5. Format for training
6. Validate with small model
