---
title: "Function Calling"
description: "Teach SLMs to use tools, APIs, and structured data outputs"
---

import NotebookWidget from '../../../../../components/NotebookWidget.astro';

<NotebookWidget 
  notebookPath="notebooks/function_calling.ipynb"
  title="Function Calling with Ollama"
  description="Learn how to make SLMs use tools and output structured JSON."
/>

# Function Calling

**What is Function Calling?** It's teaching your SLM to use tools - like calculators, APIs, databases, or any function you write. Instead of guessing or hallucinating, the model can actually call real functions to get real data.

**Why it matters:** This is how you build agents that can interact with the real world, not just generate text.

## How Function Calling Works: Step by Step

**Without Function Calling:**
```
User: "What's the weather in Tokyo?"
Model: "I don't have access to real-time weather data..."
       ↑ Model can't access external information
```

**With Function Calling:**
```
User: "What's the weather in Tokyo?"
Model: "I need to check the weather. Let me call the weather function."
       ↓
Model outputs: {"tool": "get_weather", "args": {"city": "Tokyo"}}
       ↓
Your code executes: get_weather("Tokyo") → "22°C, Sunny"
       ↓
Model: "The weather in Tokyo is 22°C and sunny."
       ↑ Model now has real data!
```

### The Complete Flow

```
┌─────────────────────────────────────────────────────────────────┐
│  STEP 1: User asks a question                                   │
│  ─────────────────────────────                                  │
│  User: "What's the weather in Tokyo?"                           │
│           │                                                     │
│           ▼                                                     │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │  STEP 2: Model analyzes the question                     │  │
│  │  ────────────────────────────────                         │  │
│  │  SLM thinks: "I need real-time weather data.             │  │
│  │              I should use the get_weather function."      │  │
│  └──────────────────────────────────────────────────────────┘  │
│           │                                                     │
│           ▼                                                     │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │  STEP 3: Model outputs structured JSON                   │  │
│  │  ───────────────────────────────                          │  │
│  │  {                                                        │  │
│  │    "tool": "get_weather",                                 │  │
│  │    "args": {"city": "Tokyo"}                              │  │
│  │  }                                                        │  │
│  └──────────────────────────────────────────────────────────┘  │
│           │                                                     │
│           ▼                                                     │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │  STEP 4: Your code executes the function                 │  │
│  │  ────────────────────────────────                         │  │
│  │  result = get_weather("Tokyo")                            │  │
│  │  → "22°C, Sunny"                                          │  │
│  └──────────────────────────────────────────────────────────┘  │
│           │                                                     │
│           ▼                                                     │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │  STEP 5: Model generates final answer                    │  │
│  │  ───────────────────────────────                          │  │
│  │  "The weather in Tokyo is 22°C and sunny."               │  │
│  └──────────────────────────────────────────────────────────┘  │
│                                                                 │
│  ✅ Model has real data, not hallucinations!                   │
└─────────────────────────────────────────────────────────────────┘
```

**Key Benefits:**
- ✅ Access to real-time data (weather, stock prices, etc.)
- ✅ Can perform calculations accurately
- ✅ Can interact with databases and APIs
- ✅ Reduces hallucinations by using real data

## Supported Models (2026)

Not all models support function calling. Top SLM choices:

| Model | Quality | Method |
|-------|---------|--------|
| **Phi-4** | ⭐⭐⭐⭐⭐ | Native support |
| **Qwen2.5-7B** | ⭐⭐⭐⭐⭐ | Native, very reliable |
| **Mistral 7B v0.3** | ⭐⭐⭐⭐ | Native |
| **Llama 3.2** | ⭐⭐⭐⭐ | Native/Prompted |
| **SmolLM2** | ⭐⭐⭐ | Prompt engineering needed |

## Implementation with Ollama

Ollama supports native tool calling with compatible models.

### 1. Define the Tool

```python
import ollama

# Define the tool schema (OpenAI format)
weather_tool = {
  'type': 'function',
  'function': {
    'name': 'get_current_weather',
    'description': 'Get the current weather for a city',
    'parameters': {
      'type': 'object',
      'properties': {
        'city': {
          'type': 'string',
          'description': 'The name of the city',
        },
        'format': {
          'type': 'string',
          'enum': ['celsius', 'fahrenheit'],
          'description': 'The temperature unit',
        },
      },
      'required': ['city'],
    },
  },
}
```

### 2. Implement the Function

```python
def get_current_weather(city, format='celsius'):
    # In real life, call an API here
    if city.lower() == 'tokyo':
        return '{"temp": 22, "condition": "sunny"}'
    elif city.lower() == 'london':
        return '{"temp": 15, "condition": "cloudy"}'
    else:
        return '{"temp": 20, "condition": "unknown"}'

# Map name to function
available_functions = {
    'get_current_weather': get_current_weather
}
```

### 3. Chat with Tools

```python
response = ollama.chat(
    model='qwen2.5:7b',
    messages=[{'role': 'user', 'content': 'What is the weather in Tokyo?'}],
    tools=[weather_tool] # Pass available tools
)

# Check if model wants to call a tool
if response['message'].get('tool_calls'):
    for tool in response['message']['tool_calls']:
        
        # Get function info
        function_name = tool['function']['name']
        function_args = tool['function']['arguments']
        
        print(f"Model calling: {function_name}({function_args})")
        
        # Execute function
        function_to_call = available_functions[function_name]
        function_response = function_to_call(**function_args)
        
        # Add tool output back to conversation
        final_response = ollama.chat(
            model='qwen2.5:7b',
            messages=[
                {'role': 'user', 'content': 'What is the weather in Tokyo?'},
                response['message'], # The tool call request
                {
                    'role': 'tool',
                    'content': function_response,
                },
            ],
        )
        
        print("Final Answer:", final_response['message']['content'])
```

## Structured Output (JSON)

Sometimes you just want data extraction, not a tool loop.

```python
from pydantic import BaseModel

class UserInfo(BaseModel):
    name: str
    age: int
    interests: list[str]

# Use libraries like 'instructor' or 'outlines' for strict JSON
import instructor
from openai import OpenAI

client = instructor.patch(OpenAI(
    base_url="http://localhost:11434/v1",
    api_key="ollama"
))

user = client.chat.completions.create(
    model="phi3.5",
    messages=[
        {"role": "user", "content": "Extract info: John is 30 and loves AI and coding."}
    ],
    response_model=UserInfo,
)

print(user.name)       # John
print(user.interests)  # ['AI', 'coding']
```

## Best Practices

1.  **Clear Descriptions**: The model reads the `description` field to know *when* to use a tool. Be verbose.
2.  **Simple Signatures**: Keep arguments simple (strings, numbers). Avoid complex objects.
3.  **Error Handling**: If the tool fails, feed the error back to the model so it can try again.
4.  **System Prompt**: Tell the model "You have access to tools. Use them whenever you need."

## Next Steps

- [Build Agents](/slmhub/docs/learn/concepts/agents/) - Use tools in loops
- [RAG Guide](/slmhub/docs/learn/concepts/rag/) - Use search as a tool
