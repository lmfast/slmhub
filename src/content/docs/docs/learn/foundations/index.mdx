---
title: Foundations
description: Core concepts of Small Language Models.
tableOfContents: false
---

Explore the fundamental concepts behind Small Language Models (SLMs) and Large Language Models (LLMs).

## Core Concepts

- **[Neural Networks Basics](./01-neural-networks-basics)**: Understanding the building blocks.
- **[Transformer Architecture](./02-transformer-architecture)**: The architecture that changed everything.
- **[Feedforward & Normalization](./03-feedforward-normalization)**: Key components of the Transformer.
- **[Complete Transformer Block](./04-complete-transformer-block)**: Putting it all together.
- **[Tokenization](./05-tokenization)**: How models understand text.
- **[KV Cache](./06-kv-cache)**: Optimizing inference.
- **[Hardware & GPU Basics](./07-hardware-gpu-basics)**: The hardware that powers AI.
- **[Quantization Methods](./08-quantization-methods)**: Making models smaller and faster.
- **[Training Optimizations](./09-training-optimizations)**: Efficient training techniques.
- **[Scaling Laws](./10-scaling-laws)**: Understanding model performance scaling.
