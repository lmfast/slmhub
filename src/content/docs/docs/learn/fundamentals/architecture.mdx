---
title: "SLM Architecture Explorer"
description: "This notebook helps explore the internal structure of Small Language Models using Hugging Face Transformers. Guide: [SLM Architecture](https://slmhub...."
---
import NotebookWidget from '../../../../../components/NotebookWidget.astro';

<NotebookWidget 
  notebookPath="notebooks/architecture.ipynb"
  title="SLM Architecture Explorer"
  description="This notebook helps explore the internal structure of Small Language Models using Hugging Face Transformers. Guide: [SLM Architecture](https://slmhub...."
/>



This notebook helps explore the internal structure of Small Language Models using Hugging Face Transformers. Guide: [SLM Architecture](https://slmhub.gitbook.io/slmhub/docs/learn/fundamentals/architecture).

## 1. Setup


```python
!pip install transformers accelerate
```

## 2. Inspect a Model Configuration
We can see the layers, heads, and dimensions without downloading the full weights.

```python
from transformers import AutoConfig

model_id = "microsoft/Phi-3-mini-4k-instruct"

config = AutoConfig.from_pretrained(model_id)
print(config)

print("\n--- Key Architecture Details ---")
print(f"Hidden Size (d_model): {config.hidden_size}")
print(f"Number of Layers: {config.num_hidden_layers}")
print(f"Attention Heads: {config.num_attention_heads}")
print(f"Vocab Size: {config.vocab_size}")
```

## 3. Compare with a Larger Model
Let's compare Phi-3 (3.8B) with Llama-2-7b.

```python
llama_config = AutoConfig.from_pretrained("meta-llama/Llama-2-7b-hf")

print(f"Phi-3 Layers: {config.num_hidden_layers} vs Llama-2 Layers: {llama_config.num_hidden_layers}")
print(f"Phi-3 Hidden: {config.hidden_size} vs Llama-2 Hidden: {llama_config.hidden_size}")
```

