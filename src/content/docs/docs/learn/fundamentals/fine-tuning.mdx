---
title: "Fine-Tuning"
description: "Adapt SLMs to your specific tasks with efficient techniques like LoRA and QLoRA"
---

<a href="https://colab.research.google.com/github/lmfast/slmhub/blob/main/notebooks/fine_tuning.ipynb" target="_blank" rel="noopener noreferrer" class="colab-button">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" />
</a>

# Fine-Tuning

Fine-tuning adapts a pre-trained model to your specific task. With LoRA, you can fine-tune a 7B model on a single GPU.

## When to Fine-Tune

```
┌─────────────────────────────────────────────────────────────────┐
│  PROMPTING vs FINE-TUNING                                       │
│  ────────────────────────                                       │
│                                                                 │
│  Start with PROMPTING:                                          │
│    "You are a helpful assistant that..."                        │
│                                                                 │
│  If prompting fails → FINE-TUNE:                               │
│    - Need specific output format                                │
│    - Domain knowledge required                                  │
│    - Consistent behavior needed                                 │
│    - Have training data (100+ examples)                         │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

## LoRA: The Modern Approach

**Low-Rank Adaptation** - freeze the model, train tiny adapters:

```
┌─────────────────────────────────────────────────────────────────┐
│  FULL FINE-TUNING (Old)         LoRA (Modern)                  │
│  ──────────────────────         ─────────────                  │
│                                                                 │
│  Update ALL weights             Freeze original weights         │
│  ████████████████████           ████████████████████            │
│  ████████████████████           ████████████████████            │
│  ████████████████████           ████████████████████            │
│                                                                 │
│                                 Add small adapters:             │
│                                 ▓▓ (trainable)                  │
│                                                                 │
│  7B parameters trained          ~10M parameters trained         │
│  Memory: 112 GB                 Memory: 8 GB                    │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### How LoRA Works

```
┌─────────────────────────────────────────────────────────────────┐
│  ORIGINAL LAYER                                                 │
│  ──────────────                                                 │
│                                                                 │
│  Input ──▶ [Weight Matrix W] ──▶ Output                        │
│            (4096 × 4096)                                        │
│            16 million params                                    │
│            FROZEN                                               │
│                                                                 │
│  LoRA ADDITION                                                  │
│  ─────────────                                                  │
│                                                                 │
│  Input ──▶ [W frozen] ──┬──▶ Output                            │
│              │          │                                       │
│              │          +                                       │
│              │          │                                       │
│              └──▶ [A] ──▶ [B] ──┘                               │
│                 (4096×16) (16×4096)                             │
│                  65K  +  65K = 130K params                      │
│                  TRAINABLE                                      │
│                                                                 │
│  Result: Train 0.8% of parameters, get 95%+ of full fine-tune  │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

## QLoRA: LoRA + Quantization

Fine-tune a 7B model on 8GB VRAM:

```python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
import torch

# Step 1: Load model in 4-bit
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True
)

model = AutoModelForCausalLM.from_pretrained(
    "microsoft/Phi-4",
    quantization_config=bnb_config,
    device_map="auto"
)

# Step 2: Prepare for training
model = prepare_model_for_kbit_training(model)

# Step 3: Add LoRA adapters
lora_config = LoraConfig(
    r=16,                  # Rank (size of adapters)
    lora_alpha=32,         # Scaling factor
    target_modules=[       # Which layers to adapt
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ],
    lora_dropout=0.05,
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
# trainable: 0.1% of total
```

## Training Dataset Format

```json
{"instruction": "Summarize:", "input": "Long text...", "response": "Summary"}
{"instruction": "Translate to French:", "input": "Hello", "response": "Bonjour"}
{"instruction": "Fix grammar:", "input": "me go store", "response": "I went to the store"}
```

## Complete Training Example

```python
from transformers import AutoTokenizer, TrainingArguments
from trl import SFTTrainer
from datasets import load_dataset

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-4")
tokenizer.pad_token = tokenizer.eos_token

# Load dataset
dataset = load_dataset("json", data_files="train.jsonl")

# Format function
def format_example(example):
    return f"""### Instruction:
{example['instruction']}

### Input:
{example['input']}

### Response:
{example['response']}"""

# Training arguments
training_args = TrainingArguments(
    output_dir="./output",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    fp16=True,
    logging_steps=10,
    save_strategy="epoch",
    optim="paged_adamw_8bit",
    gradient_checkpointing=True,
)

# Train
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset["train"],
    tokenizer=tokenizer,
    args=training_args,
    formatting_func=format_example,
    max_seq_length=2048,
)

trainer.train()

# Save adapter
model.save_pretrained("./my-adapter")
```

## Hyperparameter Guide

```
┌─────────────────────────────────────────────────────────────────┐
│  LoRA RANK (r)                                                  │
│  ─────────────                                                  │
│                                                                 │
│  r=8    ●─────────────────────────  Minimal, fast              │
│  r=16   ●─────────────────────────  Good balance ← Start here  │
│  r=32   ●─────────────────────────  Better quality             │
│  r=64   ●─────────────────────────  Best quality, more memory  │
│                                                                 │
│  Higher rank = More capacity = Better adaptation               │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────┐
│  LEARNING RATE                                                  │
│  ─────────────                                                  │
│                                                                 │
│  Full fine-tune:  1e-5 to 5e-5                                 │
│  LoRA:            1e-4 to 3e-4  ← Higher is OK                 │
│  QLoRA:           2e-4 to 3e-4                                 │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────┐
│  TARGET MODULES                                                 │
│  ──────────────                                                 │
│                                                                 │
│  Minimum (fastest):                                             │
│    target_modules = ["q_proj", "v_proj"]                       │
│                                                                 │
│  Recommended:                                                   │
│    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]   │
│                                                                 │
│  Maximum (best quality):                                        │
│    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",   │
│                      "gate_proj", "up_proj", "down_proj"]       │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

## Using Your Fine-Tuned Model

```python
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load base model
base_model = AutoModelForCausalLM.from_pretrained(
    "microsoft/Phi-4",
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

# Load your adapter
model = PeftModel.from_pretrained(base_model, "./my-adapter")

# Use it
tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-4")
inputs = tokenizer("Your prompt", return_tensors="pt").to("cuda")
outputs = model.generate(**inputs)
```

## Merge Adapter for Faster Inference

```python
# Merge LoRA into base model
merged_model = model.merge_and_unload()

# Save merged model (larger file, faster inference)
merged_model.save_pretrained("./merged-model")
```

## Common Issues

```
┌─────────────────────────────────────────────────────────────────┐
│  OUT OF MEMORY                                                  │
│  ─────────────                                                  │
│                                                                 │
│  1. Reduce batch size: per_device_train_batch_size=1           │
│  2. Increase gradient accumulation: gradient_accumulation=16   │
│  3. Enable gradient checkpointing: gradient_checkpointing=True │
│  4. Use 4-bit: load_in_4bit=True                               │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────┐
│  POOR QUALITY                                                   │
│  ────────────                                                   │
│                                                                 │
│  1. Increase rank: r=32 or r=64                                │
│  2. More target modules                                         │
│  3. More training data (1000+ examples)                        │
│  4. Longer training: num_train_epochs=5                        │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

## Key Takeaways

1. **Use LoRA/QLoRA** - Full fine-tuning rarely needed
2. **Start with r=16** - Increase if quality insufficient
3. **Quality data matters** - 1000 good examples > 10000 bad
4. **Test on your task** - Evaluate before deploying
5. **Merge for production** - Faster inference

## Next Steps

- [Embeddings](/slmhub/docs/learn/fundamentals/embeddings/) - For RAG systems
- [RAG](/slmhub/docs/learn/concepts/rag/) - Combine with retrieval
- [Deployment](/slmhub/docs/deploy/) - Production guides
